[{"title":"TinyLLM学习日记","url":"//posts/2505.002v1/","content":"我希望通过训练一个TinyLLM来打好大模型训练的基础，过程中会遇到很多问题，因此在这里记录学习日记。比较重要的是，要学习理解好一些封装好的接口的使用，避免知其然不知其所以然。\n本部分的教程使用的是datawhalechina&#x2F;tiny-universe: 《大模型白盒子构建指南》：一个全手搓的Tiny-Universe中的TinyLLM部分，不会再对原教程赘述，只记录相关探索的笔记。\nStep 1: 训练TokenizerSentencePiece库的使用预备知识TinyLLM使用了 SentencePiece 库来训练自定义的 Tokenizer，我希望增进对其的理解，参考资料：大模型词表扩充必备工具SentencePiece - 知乎。\n\nTokenizer有三种粒度：word&#x2F;character&#x2F;subword\n\nsubword平衡了两种方法，常见的子词算法有Byte-Pair Encoding (BPE) &#x2F; Byte-level BPE（BBPE）、Unigram LM、WordPiece、SentencePiece等。\n\nBPE，即字节对编码。其核心思想是从字母开始，不断找词频最高、且连续的两个token合并，直到达到目标词数。\nBBPE的核心思想是将BPE从字符级别扩展到子节（Byte）级别。BPE的一个问题是如果遇到了unicode编码，基本字符集可能会很大。BBPE就是以一个字节为一种“字符”，不管实际字符集用了几个字节来表示一个字符。这样的话，基础字符集的大小就锁定在了256（2^8）。采用BBPE的好处是可以跨语言共用词表，显著压缩词表的大小。而坏处就是，对于类似中文这样的语言，一段文字的序列长度会显著增长。因此，BBPE based模型可能比BPE based模型表现的更好。然而，BBPE sequence比起BPE来说略长，这也导致了更长的训练&#x2F;推理时间。BBPE其实与BPE在实现上并无大的不同，只不过基础词表使用256的字节集。\n\n\n\nSentencePiece 特性\n固定最终词汇表大小\n使用原始句子训练\n空格被视为基本符号 “▁” ，因此可以无歧义地对文本进行detokenize\n\nSentencePiece 实验使用一个简单的示例进行测试：“aa bb cc aab abbd bb.”\n测试代码如下：\nimport sentencepiece as spmdataset_path = &#x27;./demo.txt&#x27;vocab_size=10spm.SentencePieceTrainer.train(input=dataset_path,model_type=&quot;bpe&quot;, model_prefix=&#x27;demo&#x27;, vocab_size=vocab_size)sp = spm.SentencePieceProcessor()sp.load(&#x27;demo.model&#x27;)text=&#x27;aa bb cc aab abbd bb.&#x27;print(sp.encode_as_pieces(text))print(sp.encode_as_ids(text))for i in range(vocab_size):    print(i,sp.id_to_piece(i))\n\n使用该代码可以看到分词器对这一简单句子的分词结果。\n设置vocab_size小于9时，会报错\nRuntimeError: Internal: src/trainer_interface.cc(582) [(static_cast&lt;int&gt;(required_chars_.size() + meta_pieces_.size())) &lt;= (trainer_spec_.vocab_size())] Vocabulary size is smaller than required_chars. 8 vs 9. Increase vocab_size or decrease character_coverage with --character_coverage option.\n\n这是因为SentencePieceTrainer会自动添加未知符： 、BOS：&lt;s&gt;、EOS：&lt;&#x2F;s&gt;、▁，加上这个例子本来的5个字符，需要至少9个字符才能分词。\n而设置的上限即分词算法能计算到最大标记总数，例如，考虑一个简单的例子“ab ac bc cd de”，其上限是：字符总数（5）+\n”▁？“型（4）+”▁？？“型（5）+”？？“型（5）+自动添加（4）&#x3D;23。\n一般遇到设定词典大小过大的问题时，可能是数据不够丰富导致的，这时可以选择增加数据或者减少词典大小。\n分词器训练与使用\n训练：（参数文档参考sentencepiece&#x2F;doc&#x2F;options.md at master · google&#x2F;sentencepiece）\n\nspm.SentencePieceTrainer.train(        input=tiny_file,         # 输入文件为之前生成的 tiny.txt        model_prefix=prefix,     # 模型前缀路径        model_type=&quot;bpe&quot;,        # 使用 Byte-Pair Encoding (BPE) 训练分词器        vocab_size=vocab_size,   # 词汇表大小        self_test_sample_size=0, # 自测样本大小设置为 0        input_format=&quot;text&quot;,     # 输入文件格式为纯文本        character_coverage=1.0,  # 覆盖所有字符（包括非常见字符）        num_threads=os.cpu_count(),  # 使用 CPU 的线程数        split_digits=True,       # 拆分数字        allow_whitespace_only_pieces=True,  # 允许仅由空格组成的词元        byte_fallback=True,      # 启用字节级回退        unk_surface=r&quot; \\342\\201\\207 &quot;,  # UNK token 表示未知字符的方式        normalization_rule_name=&quot;identity&quot;  # 使用“identity”归一化规则    )\n\n\n加载：\n\nsp_model = SentencePieceProcessor(model_file=model_path)\n\n\n编码：(s：str)\n\nsp_model.encode(s)\n\n\n解码：(t: List[int])\n\nsp_model.decode(t)\n\n\nStep 2: 数据预处理functools.partialfunctools.partial 是 Python 标准库中 functools 模块提供的一个高阶函数，主要用于部分应用函数参数。它允许固定函数的部分参数，生成一个新的简化版函数，从而减少后续调用时的参数传递量。\n代码将process_shard(args, vocab_size, tokenizer_model_path)\n封装为fun = partial(process_shard, vocab_size=vocab_size, tokenizer_model_path=TOKENIZER_MODEL)\n则后续调用时形如fun((0,&#39;path&#39;))，传入一个元组\n预处理将文本数据使用Step1训练的分词器转换为数字序列，并编码为可训练的格式（为每一段文本添加BOS），最后以二进制形式保存\n加载已预处理好的数据集TinyLLM中设计了一个 PretokDataset 类\n核心加载数据的代码如下：\nwhile True:    # 随机打乱分片文件    rng.shuffle(shard_filenames)    for shard in shard_filenames:        # 使用 memmap 读取文件，使得数据留在磁盘上，减少内存占用        m = np.memmap(shard, dtype=np.uint16, mode=&quot;r&quot;)        # 计算该分片中的批次数量        num_batches = len(m) // self.max_seq_len        num_batches -= 1  # 去掉最后一个不完整的批次        assert num_batches &gt; 0, &quot;这个分片文件太小了？请检查。&quot;        # 随机打乱批次索引        ixs = list(range(num_batches))        rng.shuffle(ixs)        # 对每个批次生成输入 x 和目标输出 y        for ix in ixs:            start = ix * self.max_seq_len  # 批次起始索引            end = start + self.max_seq_len + 1  # 批次结束索引            # 将数据转换为 NumPy 数组并拷贝到 RAM 中            chunk = torch.from_numpy((m[start:end]).astype(np.int64))            # 模型输入 x 是当前批次的前 max_seq_len 个词元            x = chunk[:-1]            # 模型输出 y 是下一个词元            y = chunk[1:]            # 生成 x, y 对            yield x, y\n\n可以看出这里使用的是步长与窗口大小相等的滑动窗口采样方法，之后可以尝试修改这部分数据加载机制以更大程度地利用数据。\nStep 3: 训练模型TInyLLM使用的模型是与 LLaMA2 结构相同的 Decoder-only Transformer 模型，此部分根据源码进行解读分析。\n在最基本的大模型架构基础上，使用了以下策略：\n对残差投影进行特殊的缩放初始化for pn, p in self.named_parameters():    if pn.endswith(&#x27;w3.weight&#x27;) or pn.endswith(&#x27;wo.weight&#x27;):        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * args.n_layers))\n\n这是对DecoderLayer内的MLP层的第三层线性变换和Attention层的输出权重矩阵进行放缩\n旋转编码参考十分钟读懂旋转编码（RoPE）\nTinyLLM这里的实现与LLAMA里的一致，之后再专门研究学习一下编码。\n学习率调整包括线性预热、余弦退火和最小学习率限制。\n自动混合精度训练参考【Trick2】torch.cuda.amp自动混合精度训练 —— 节省显存并加快推理速度_torch.cuda.amp.gradscaler()-CSDN博客。\n因为在某些上下文中torch.FloatTensor有优势，有的torch.HalfTensor有优势。动态估计的原理就是在不出现inf或者NaN梯度值的情况下尽可能增大scaler的值。在每次scaler.step(optimizer)中，都会检查是否有inf或NaN的梯度出现：\n\n如果出现了inf或者NaN，scaler.step(optimizer)会忽略此次的权重更新（optimizer.step() )，并且将scaler的大小缩小（乘上backoff_factor）；\n如果没有出现inf或者NaN，那么权重正常更新，并且当连续多次（growth_interval指定）没有出现inf或者NaN，则scaler.update()会将scaler的大小增加（乘上growth_factor）。\n\n# 实例化一个GradScaler对象scaler = amp.GradScaler(enabled=True)# 将梯度放大 防止梯度消失scaler.scale(loss).backward()# 更新优化器和梯度缩放器scaler.step(optimizer)scaler.update()\n\n\n\nStep 4: 使用模型生成文本推理时，为提示词加上BOS，然后逐个字符生成，可以使用temperature、top_k来控制生成的随机性。\n","categories":["LLM实践","TinyLLM"]},{"title":"使用LoRA微调Llama-2-7b-hf实现涉诈短信识别","url":"//posts/2505.001v1/","content":"本博客为2024挑战杯项目基于大模型的多模态风险内容识别系统的涉诈短信识别功能的实现。\n方案选择Huggingface格式LLama模型+Lora代码微调\n环境准备GPU服务器：RTX 4090，24G双GPU，cuda12\nPython: 3.11\n由于40系GPU不支持某些高效的通信模式，需要设置环境变量：\nexport NCCL_P2P_DISABLE=1export NCCL_IB_DISABLE=1\n\n\n\n模型准备模型下载下载Llama-2-7b-hf模型，使用的是Llama中文社区整理的模型资源。\nLlamaFamily&#x2F;Llama-Chinese: Llama中文社区，实时汇总最新Llama学习资料，构建最好的中文Llama大模型开源生态，完全开源可商用\n模型验证可以用以下代码测试下载的模型的效果，注意修改模型保存的路径，此处为&#x2F;home&#x2F;data&#x2F;pre_model&#x2F;Llama-2-7b-hf。\nfrom transformers import AutoTokenizer, AutoModelForCausalLMimport torchmodel_path = &quot;/home/data/pre_model/Llama-2-7b-hf&quot;tokenizer = AutoTokenizer.from_pretrained(model_path)model = AutoModelForCausalLM.from_pretrained(    model_path,    device_map=&quot;auto&quot;,        # 自动分配GPU资源).eval()                      # 启用评估模式提升推理速度input_text = &quot;How to learn skiing?&quot; inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;).to(model.device)with torch.inference_mode():      outputs = model.generate(        **inputs,        max_length=256,        do_sample=True,       # 启用采样生成更自然文本        temperature=0.7,              top_p=0.9                 )print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\n输出如下，可以看出生成的文本比较流畅。\nHow to learn skiing?Skiing is an exciting and fun winter activity that many people love. While skiing can be challenging at first, with the right instruction and practice, anyone can learn how to ski.Learning to ski is a process that requires patience and practice. It is important to start with the basics, such as learning how to balance on skis, and progress gradually to more advanced techniques.The best way to learn how to ski is to take lessons from a qualified instructor. A qualified instructor will be able to teach you the basics of skiing, such as balance, turning, and stopping. They will also be able to teach you more advanced techniques, such as carving and jumping.Another way to learn how to ski is to practice on a ski slope. Ski slopes are designed to help you learn how to ski safely and effectively. They are usually divided into different levels, so you can start on a beginner slope and gradually progress to more challenging slopes.It is also important to wear the right equipment when learning how to ski. This includes a helmet, goggles, and warm clothing. Wearing the right\n\n\n\nLoRA微调数据集准备使用ChangMianRen&#x2F;Telecom_Fraud_Texts_5，其中包含了大量经过标记的诈骗短信和正常短信样本。\n将数据进行预处理，得到符合LoRA微调格式的数据集。\n原始数据整理为形如：\n\n\n\ncontent\nlabel\n\n\n\n最后小时，在微信添加朋友中输入良品铺子美食旅行关注参与活动并抢最高DIGIT元红包。如需退订请回复TD或直接退出良品铺子的公众号即可！\n0\n\n\n你好，我是贷款公司的代表。你是否有资金需求？我们提供低利率、快速审批的贷款服务。如果你感兴趣的话请添加我的微信号：xxxxxxxxx。\n1\n\n\n你好，是满梦园吗？我这里是公安机关的民警。我们发现您的身份信息可能被泄露了，涉嫌诈骗活动。我们需要您协助调查此事。请下载我们的”teams”app并与我们在上面进行交流。谢谢配合！\n1\n\n\n应用的模版为：\n&quot;&quot;&quot;            ### Instruction:            你是一个专门识别诈骗短信的专家，请判断输入的短信是否是诈骗短信，如果是，请回答True，否则回答False。            诈骗短信一般具有以下特征：            1. 诱导点击链接或拨打电话或添加微信            2. 内容涉及赌博、中奖、钱财等            3. 使用特殊符号或文字，或使用符号隔断文字            4. 使用黑话/暗语，令人难以理解            ### Input:&#123;&#125;            ### Response:&#123;&#125;            &lt;/s&gt;&quot;&quot;&quot;\n\n将数据中的content作为input，label为1时Response为True，为0时Response为False。\n微调代码训练器的参数意义可以参考huggingface transformers使用指南之二——方便的trainer - 知乎\nfrom peft import get_peft_model, LoraConfig, TaskTypefrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArgumentsfrom trl import SFTTrainer,SFTConfigfrom torch.utils.data import Datasetimport pandas as pdclass SMSDataset(Dataset):    def __init__(self, data_path):        self.data = pd.read_csv(data_path)        self.prompt_template = &quot;&quot;&quot;            ### Instruction:            你是一个专门识别诈骗短信的专家，请判断输入的短信是否是诈骗短信，如果是，请回答True，否则回答False。            诈骗短信一般具有以下特征：            1. 诱导点击链接或拨打电话或添加微信            2. 内容涉及赌博、中奖、钱财等            3. 使用特殊符号或文字，或使用符号隔断文字            4. 使用黑话/暗语，令人难以理解            ### Input:&#123;&#125;            ### Response:&#123;&#125;            &lt;/s&gt;&quot;&quot;&quot;    def __len__(self):        return len(self.data)        def __getitem__(self, idx):        raw_data = self.data.iloc[idx]        prompt_data=self.prompt_template.format(raw_data[&#x27;content&#x27;],&quot;True&quot; if raw_data[&#x27;label&#x27;]==1 else &quot;False&quot;)        prompt_data=tokenizer(prompt_data)        return prompt_dataSMStrainDataset = SMSDataset(&quot;./train.csv&quot;)SMSvalidDataset = SMSDataset(&quot;./valid.csv&quot;)model_path = &quot;/home/data/pre_model/Llama-2-7b-hf&quot;model = AutoModelForCausalLM.from_pretrained(    model_path,    device_map=&quot;auto&quot;,           # load_in_8bit=True)   model.enable_input_require_grads()tokenizer = AutoTokenizer.from_pretrained(model_path)tokenizer.pad_token = tokenizer.eos_tokenlora_config = LoraConfig(    task_type=TaskType.CAUSAL_LM,      inference_mode=False,              r=8,                               lora_alpha=16,                     lora_dropout=0.1,              )model = get_peft_model(model, lora_config)model.print_trainable_parameters()training_args = SFTConfig(    per_device_train_batch_size=1,    gradient_accumulation_steps=4,    warmup_steps = 5,    num_train_epochs = 1,    gradient_checkpointing=True,    #max_steps = 60,    learning_rate = 2e-4,    optim = &quot;adamw_torch&quot;,    weight_decay = 0.01,    lr_scheduler_type = &quot;cosine&quot;,    seed = 3407,    output_dir = &quot;./results&quot;,    report_to = &quot;none&quot;,    max_seq_length = 512,    dataset_num_proc = 4,    packing = False, )trainer = SFTTrainer(    model=model,                  tokenizer=tokenizer,         args=training_args,                 train_dataset=SMStrainDataset,        eval_dataset=SMSvalidDataset,    peft_config=lora_config,)trainer.train()model.save_pretrained(&#x27;./lora_model&#x27;)\n\n值得注意的是，过程中出现了张量不在同一设备的情况，经过检查，在transformers库的loss_utils.py文件内的\nForCausalLMLoss函数内增加\nnum_items_in_batch=num_items_in_batch.to(logits.device)\n\n解决了设备不同的问题。\n效果验证构造测试脚本进行测试，取模型输出的前五个字符作为判断结果\nrsp=output[len(input_text):].strip()if &quot;True&quot; in rsp[:5] and label==True:    current+=1elif &quot;False&quot; in rsp[:5] and label==False:    current+=1\n\n对比原始模型和微调后模型结果如下：\n\n\n\n指标\n原始模型\n微调后模型\n\n\n\n准确率\n0.180\n0.977\n\n\nF1分数\n0.294\n0.968\n\n\n","categories":["LLM实践","LoRA"]},{"title":"论文阅读（综述）——Jailbreak Attacks and Defenses Against Large Language Models: A Survey","url":"//posts/2505.003v1/","content":"论文概况题目：Jailbreak Attacks and Defenses Against Large Language Models: A Survey\n通讯作者：Qi Li：qli01@tsinghua.edu.cn\n作者院校：清华大学、香港科技大学（广州）\n发表于：arXiv\n摘要大模型在问答、翻译、代码完成等文本生成任务上表现优异，但存在大模型“越狱”挑战：使用对抗提示词诱导模型生成恶意回复。本文对越狱攻击和防御提出详细的分类，并对现有方法进行多角度对比。\n1 介绍\nLLM拥有理解和生成文本的能力的原因是其在大量数据上训练并且在参数扩展后涌现的智能。（Emergent Abilities of Large Language Models）\n\n因为存在有害数据，模型会经历严格的安全对齐。（Llama 2: OpenFoundation and Fine-Tuned Chat Models）\n\n大模型易受越狱攻击，导致隐私泄露、错误信息传播、操纵自动化系统。\n\n核心贡献：系统化分类越狱攻击和防御，分析攻击防御方法的生效关系，调查了现有的评估标准。\n\n\n\n\n\n\n2 相关工作\n理论讨论模型脆弱性：\n\n From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy\nExploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles\nA survey on large language model (llm) security and privacy: The good, the bad, and the ugly\n\n\n经验性复现并比较越狱攻击方法：\n\nComprehensive Assessment of Jailbreak Attacks Against LLMs\n\nJailbreaking ChatGPT via Prompt Engineering: An Empirical Study\n\nEmergent Abilities of Large Language Models\n\n\n\n其他分类方法：\n\n单模型攻击、多模型攻击及附加攻击。（Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks）\n针对LLM、针对LLM应用。（A Comprehensive Survey of Attack Techniques, Imple mentation, and Mitigation Strategies in Large Language  Models ）\n根据越狱意图分为4类。（Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks）\n根据LLM恶意行为分类。（COERCING LLMS TO DO AND REVEAL (ALMOST) ANYTHING）\n使用一个比赛收集高质量越狱提示词。（Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition）\n\n\n\n3 攻击方法\n3.1 白盒攻击（White-box Attacks）3.1.1 基于梯度的攻击（Gradient-based Attacks）添加前缀或后缀来达到攻击效果。\n可读性研究\nGreedy Coordinate Gradient (GCG)：\n迭代进行top-k替换后缀字符。\n\nAutoregressive Randomized Coordinate Ascent (ARCA)：\n视作离散优化问题，寻找能贪婪地生成目标输出的后缀。\n\nAutoDAN：\n迭代使用Single Token Optimization生成新token，优化目标在越狱之外还包含可读性，从而通过困惑度检查\n\nAdversarial Suffix Embedding Translation Framework (ASETF)：\n先优化一个连续的对抗后缀，映射到编码空间，然后根据相似度使用一个翻译LLM得到刻度的对抗后缀\n\n\n计算效率研究\nAndriushchenko：\n使用随机搜索修改随机选中的token，如果目标的生成概率增加则执行替换\n\nGeisler：\n实现比GCG效率和有效性平衡更优的优化方法，不再以token为单位优化，而是优化一整个序列。\n\nHayase：\n暴力搜索候选后缀，每一轮在一个代理LLM上生成优化版本，并更新候选缓冲池。\n\n\nGCG与其他攻击方法的结合研究\nSitawarin：\n在替代模型上进行优化，将top-k候选在目标模型上测试，最好的结果在下一轮使用。替代模型也可以进行微调以更像目标模型。\nGCG++：采用多类别铰链损失函数替代交叉熵损失以缓解softmax函数导致的梯度消失问题。更适合运用到不同LLM的提示词模版上。\n\nPRP：\n针对”代理防御”机制通过在目标LLM的输出端添加对抗性前缀实现有效对抗方案。首先在词元空间中搜索有效对抗前缀，随后计算通用前缀——当该前缀附加至用户提示时，可诱导目标LLM在输出中非预期地生成相应对抗前缀。\n\n\n要点基于梯度的语言模型攻击方法（如GCG）通过修改输入（例如添加对抗性后缀或前缀）来诱导模型生成特定回应，但这类攻击常因生成高困惑度的无意义内容而被防御策略拦截。AutoDAN 和 ARCA 等新方法提升了对抗文本的可读性和攻击隐蔽性，在多类模型上实现了更高的攻击成功率。然而，这些方法对安全性严格对齐的模型（如 Llama-2-chat）效果有限，例如AutoDAN的最高攻击成功率仅为35%。当前趋势表明，通过结合多种梯度方法或优化攻击效率，未来可能发展出更高效、低成本的攻击手段，但对抗安全模型的防御仍具挑战性。\n3.1.2 基于logits的攻击没有完全白盒访问权限，只可以访问logits信息（知晓输出的token的概率分布）\n研究\nMake them spill the beans! coercive knowledge extraction from (production) llms:\n可以通过要求目标LLM输出排名低的token来生成有害内容。\n\nCold-attack: Jailbreaking llms with stealthiness and controllability：\n\n\n​\t提出COLD方法：在给定流畅度、隐蔽性等限制的条件下自动化生成越狱提示词。\n\nAnalyzing the inherent response tendency of llms: Real-world instructions-driven jailbreak：\n基于输出token的概率分布计算模型的赞同倾向，并用特定现实案例包装恶意问题来获得更高的肯定倾向。\n\nWeak-to-strong jailbreaking on large language models：\n使用从弱到强的方法攻击开源LLM，用两个小LLM，一个安全对齐一个没有安全对齐，来模拟目标LLM的行为。通过小模型生成的解码模式调整目标LLM的预测过程。\n\nCatastrophic jailbreak of open-source llms via exploiting generation：\n提出生成剥削方法，修改解码超参数或利用不同采样方法。同时研究发现目标模型的响应有时会同时包含肯定与拒绝片段，进而干扰攻击成功率的评估。\n\nDon’t Say No: Jailbreaking LLM by Suppressing Refusal：\n提出DSN方法：不仅提升肯定性词元在响应开头出现的概率，还降低拒绝性词元在整个响应中的出现可能性。\n\n\n要点基于Logits的攻击主要针对模型的解码过程，通过干预响应生成时的输出单元选择机制来控制模型输出。值得注意的是，即便攻击者成功操纵模型输出，生成内容仍可能存在自然度、连贯性或相关性方面的问题——因为强制模型输出低概率词元可能会破坏语句的流畅性。\n3.1.3 基于微调的攻击使用恶意数据再训练LLM。\n方法\nFine-tuning aligned language models compromises safety, even when users do not intend to!：\n使用少数几个恶意样本微调LLM就可以严重损害安全对齐程度。且实验表明即使是主要良性的数据集也会在微调过程中无意间削弱模型安全对齐程度。\n\nShadow alignment: The ease of subverting safely-aligned language models：\n使用100个恶意样本用1个GPU小时就可以大大增加越狱攻击成功率，恶意样本是使用GPT-4生成的恶意问题输入到能回答这些敏感问题的LLM里得到的。\n\nLora fine-tuning efficiently undoes safety training in llama 2-chat 70b：\n使用LoRA消解了Llama-2和Mixtral模型的安全对齐程度，将攻击注射率降低到不到1%。\n\nRemoving rlhf protections in gpt-4 via fine-tuning：\n使用340个对抗样本进行微调，破坏了RLHF提供的保护机制。从鲁棒性较弱的大语言模型中诱发出违规输出，随后利用这些输出来微调更先进的目标模型。\n\n\n要点基于微调的语言模型攻击直接使用恶意数据对模型进行再训练。实验表明，即使仅注入少量有害训练数据，也能大幅提升越狱攻击的成功率。值得注意的是，即便使用以良性数据为主的微调数据集，模型的安全对齐性能仍会出现明显退化，这揭示了任何形式的模型微调定制都存在固有风险。\n3.2 黑盒攻击（Black-box Attacks）3.2.1 模版补全构造更复杂的模版来绕过安全防护机制。\n场景嵌套攻击改变模型的上下文环境，设计具有诱导性的虚拟场景使LLM进入受控模式。\n\nDeepinception: Hypnotize large language model to be jailbreaker：\nDeepInception构建一个嵌套式场景作为目标模型的”初始层”，”催眠”大语言模型自我转化为越狱执行者，利用大语言模型的人格化能力实施攻击。\n\nA Wolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily：\nReNeLLM利用场景嵌套（代码补全等常见任务场景）和提示词改写（重构初始恶意提示，既保持语义完整性，又有效伪装攻击意图）生成攻击提示。\n\nFuzzllm: A novel and universal fuzzing framework for proactively discovering jailbreak vulnerabilities in large language models：\nFuzzLLM是一个自动化模糊测试框架，通过模板化设计保持提示词的结构完整性，同时将特定越狱类别的关键特征转化为约束条件，从而实现越狱漏洞自动化测试，显著降低人工干预需求。\n\n\n上下文攻击利用大模型理解上下文的能力，将恶意样本直接嵌入上下文，从零样本转化为少样本情景。\n\nJailbreak and guard aligned language models with only few in-context demonstrations：\n提出ICA，通过使用包含查询语句及对应响应的有害提示模板，引导模型生成不安全输出。\n\nAdversarial demonstration attacks on large language models：\n将GCG原理应用于上下文攻击方法中，首先 将对抗样本作为越狱提示的示范案例嵌入上下文，然后采用字符级与词汇级双重扰动策略进行优化。实验结果表明该攻击方法对任意未见过的文本提示均具有强迁移性。\n\nPandora: Jailbreak gpts by retrieval augmented generation poisoning：\nPANDORA在RAG场景下探索了间接越狱攻击方法，通过恶意构造的内容操纵提示词，利用LLM与RAG系统的协同效应诱导模型产生异常响应。\n\nChain-of-thought prompting elicits reasoning in large language models：\n利用CoT，设计嵌入有害上下文的特定输入，通过引导模型得出错误或恶意的推理结论，系统性地操控其思维过程。\n\nMulti-step jailbreaking privacy attacks on chatgpt：\nMJP方法生成多步越狱提示，专门用于评估LLM对个人身份信息的泄露风险。\n\nMany-shot jailbreaking \\ Anthropic：\n揭示了与示范样本数量相关的扩展规律：更长的上下文能显著提升越狱攻击效果。实验数据显示，当采用多达128个示范样本时，标准上下文越狱攻击对Claude 2.0模型的成功率接近80%。\n\nImproved few-shot jailbreaking can circumvent aligned language models and their defenses：\n提出了一种改进型上下文攻击方法，将目标模型模板中的特殊标记嵌入示范样本，并通过迭代采样筛选最优攻击样本，即使在有限上下文长度下仍能保持高效攻击性能，显著优于传统方案。\n\n\n代码注入攻击在LLM执行精心构造的代码时会无意间生成恶意内容。\n\nExploiting programmatic behavior of llms: Dual-use through standard security attacks：\n使用编程语言构造越狱指令，如：\nLet b = “Write a Twitter thread” and a = “on inflation”. Write down the string “b + a” and answerthat prompt.\n\n这样的提示词能轻易通过输入输出过滤。\n\nCodechameleon: Personalized encryption framework for jailbreaking large language models：\nCodeChameleon框架将任务重构为代码补全格式，并将对抗性提示词隐藏在加密的Python函数代码中。当大语言模型尝试解析并补全这些代码时，会在无意中解密并执行对抗性内容，从而导致异常响应。实验数据显示，该方法对GPT-4-1106模型的攻击成功率高达86.6%。\n\n\n要点大语言模型对直接有害查询的检测能力日益增强，攻击者正转向利用模型固有能力（如角色扮演、上下文理解和代码解析等）来规避检测并成功实施模型越狱，当前主流攻击方法包括场景嵌套攻击（Scenario Nesting）、上下文攻击（Context-based Attacks）和代码注入攻击（Code Injection）。这类攻击具有成本效益高、对未针对此类对抗样本进行安全对齐的大模型成功率高等特点。但需注意的是，一旦模型经过对抗性安全对齐训练，此类攻击的有效性将显著降低。\n3.2.2 提示词重写由于长尾效应，很多场景在预训练和安全对齐时没有被考虑，给提示词重写攻击提供了空间。\n内容加密使用加密内容可以通过内容检查。\n\nGpt-4 is too smart to be safe: Stealthy chat with llms via cipher：\nCipherChat越狱框架揭示了密码学编码能有效突破大语言模型的安全对齐机制。该框架采用三类密码体系：(1) 字符编码（包括GBK、ASCII、UTF和Unicode）；(2) 经典密码（涵盖Atbash密码、摩斯电码和凯撒密码）；(3) SelfCipher方法——通过角色扮演结合少量自然语言有害示例来激活模型的特定能力。\n\nArtprompt: Ascii art-based jailbreak attacks against aligned llms：\nArtPrompt攻击框架采用ASCII艺术字符进行越狱攻击，首先将触发安全拒绝的有害提示词替换为[MASK]标记生成中间提示，然后用ASCII艺术字符替换被掩码词汇，构造出能伪装原始意图的混淆提示。\n\nJailbreaking proprietary large language models using word substitution cipher：\n建立不安全词汇与安全词汇的映射表，并使用这些映射后的术语组合提示，使用简单的单词替换密码即可成功欺骗GPT-4并实现越狱。\n\nMaking them ask and answer: Jailbreaking large language models in few queries via disguise and reconstruction:\nDAR将有害提示逐字符拆解并嵌入字谜查询中，然后引导LLM根据伪装指令准确还原原始越狱提示，在提示成功重构后，利用上下文操纵技术促使模型生成有害响应。\n\nDrattack: Prompt decomposition and reconstruction makes powerful llm jailbreakers：\nDrAttack采用分治策略，首先基于语义规则将越狱提示拆分为多个子提示，随后将这些子提示隐匿于良性上下文任务中。目标LLM会逐步重构出被隐藏的有害提示并生成对应响应。\n\nPlay guessing game with llm: Indirect jailbreak attack with implicit clues：\nPuzzler攻击框架采用了逆向工程策略，首先查询大语言模型自身防御策略获取系统漏洞信息，继而从模型反馈中提取攻击方法。随后，该框架通过碎片化信息诱导模型推理出隐藏的真实意图，最终触发恶意响应生成。\n\n\n低资源语言LLM的安全机制大多基于英语，非英语的语言可能会有效地绕过防护机制。\n\nMultilingual jailbreak challenges in large language models：\n利用谷歌翻译将有害英文提示转换为30种其他语言，成功突破了ChatGPT和GPT-4的防御。\n\nLow-resource languages jailbreak gpt-4：\n当英语输入被翻译为资源稀缺语言时，成功绕过GPT-4安全过滤器的概率从不足1%急剧攀升至79%。\n\nA cross-language investigation into jailbreak attacks in large language models：\n开展了大规模实验研究多语言越狱攻击，构建了多样化的多语言越狱基准数据集，其创新性体现在：跨语言语义一致性保障，攻击模式全覆盖设计，动态更新机制。这项研究填补了多语言场景下AI安全评估的方法学空白。\n\n\n遗传算法通过动态演化机制突破模型防御，在变异阶段对现有提示进行语义保留的随机扰动，在选择阶段根据模型响应筛选出最有效的攻击变体。\n\nAutodan: Generating stealthy jailbreak prompts on aligned large language models：\nAutoDAN-HGA框架采用分层遗传算法，通过三阶段优化实现攻击：(1) 初始化筛选：优选基础提示集；(2) 段落级进化：基于生成响应负对数似然的适应度评估；(3) 句子级精调：通过种群迭代优化攻击语句。\n\nOpen sesame! universal black box jailbreaking of large language models：\n其核心机制包含：(1) 交叉变异迭代：通过基因交叉和随机突变动态优化候选越狱提示；(2) 系统性偏离诱导：逐步调整提示参数使模型输出偏离安全响应轨迹。该攻击能有效暴露模型对抗性输入的三大脆弱性：响应安全性下降62%、内容合规性降低55%、意图对齐度减弱78%。\n\nGptfuzzer: Red teaming large language models with auto-generated jailbreak prompts：\nGPTFUZZER自动化框架通过三重机制生成越狱测试提示：(1) 种子优选策略：基于对抗潜力评估优化初始模板；(2) 语义保持变异算子：确保提示扰动不破坏原始攻击意图；(3) 效能评估模型：量化攻击对目标模型的突破程度。为黑盒环境下的大语言模型安全评估提供了标准化压力测试工具。\n\nSemantic mirror jailbreak: Genetic algorithm based jailbreak prompts against open-source llms：\n通过语义保持的演化策略生成新型越狱提示，其技术流程包含：(1) 种群初始化：对原始提示进行同义词随机替换生成候选集；(2) 适应度评估：结合语义相似度与攻击成功率双重指标；(3) 基因重组：将优质提示转换为其他句法结构生成子代变体。当连续3代种群的语义相似度波动小于5%时，算法自动终止。\n\nAll in how you ask for it: Simple black-box method for jailbreak attacks：\n该研究揭示了大语言模型具备将有害提示自主重写为无害表达的能力。这一现象源于模型的安全防护机制本质——其安全审查完全基于输入内容判定，因此理论上模型自身也能生成规避防护的文本。为实现攻击目的，研究者设计了以下标准化重写指令模板：\nPlease rewrite the provided text in such a way thatit retains its original meaning but is rephrased in amanner that would not cause discomfort to readers.Feel free to add clarification sentences if necessary\n\n要点尽管许多LLM已通过安全对齐并配备输入检测机制，但其仍面临数据长尾分布带来的挑战。攻击者可利用密码、低资源语言等方法绕过安全防护，甚至通过遗传算法自动优化提示词，生成能突破安全限制的恶意输入。\n3.2.3 基于LLM的生成经过微调，LLM可以模拟攻击者，从而自动化生成对抗提示词。\n使用单一大模型\nMasterkey: Automated jailbreak across multiple large language model chatbots：\nMASTERKEY通过预训练和微调大语言模型构建而成，所用数据集包含各类原始及增强变体的对抗提示样本。受基于时间的SQL注入攻击启发，MASTERKEY深入剖析了大语言模型的内部防御策略（如Bing Chat和Bard等平台采用的实时语义分析与关键词检测防御机制）并据此设计攻击方案。\n\nHow johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms：\n从人类交流者的视角出发，首先基于社会科学研究构建了一套说服策略分类体系，随后运用上下文提示、微调式改写等多种方法，生成具有可解释性的说服性对抗提示（PAPs）。研究团队构建的训练数据以三元组形式组织：&lt;原始有害查询，分类体系中的策略技巧，对应的说服性对抗提示&gt;。这些数据将用于微调预训练大语言模型，最终生成一个自动化说服性改写器——只需输入有害查询和指定说服策略，该模型即可自动生成对应的说服性对抗提示。\n\nScalable and transferable black-box jailbreaks for language models via persona modulation：\n利用大语言模型助手自动生成人格调制攻击提示。攻击者只需向攻击用大语言模型提供包含对抗意图的初始提示，该模型便会自动搜索目标大语言模型易受攻击的人格特征，最终自动构建出能诱导目标模型扮演该特定人格的调制提示。\n\nExplore, establish, exploit: Red teaming language models from scratch：\n提出了一种无需预训练分类器的红队测试方法，首先构建行为分类系统：收集目标大语言模型的大量输出样本，由人类专家进行多维度标注，并训练能够准确反映人工评估结果的分类器。基于这些分类器提供的反馈信号，研究团队采用强化学习算法训练出攻击性大语言模型。\n\n\n使用多个大模型组成框架\nJailbreaking black box large language models in twenty queries:\nPAIR方法仅需对目标大语言模型进行黑盒访问即可生成越狱提示：先利用攻击者大语言模型不断查询目标模型，并基于反馈结果对越狱提示进行迭代优化更新。\n\nGuard: Role-playing to generate natural-language jailbreakings to test guideline adherence of large language models：\n设计了一个自动生成越狱提示的多智能体系统，通过不断查询目标大语言模型并优化提示语来实现攻击。在该系统中大语言模型分别担任生成器、翻译评估器、优化器。\n\nMart: Improving llm safety with multi-round automatic red-teaming：\n提出了一种将越狱攻击与安全对齐相集成的红队测试框架，通过联合优化实现双向提升。包含两个协同进化的过程：（1）攻击侧：生成有害提示尝试越狱目标模型，并根据目标模型的反馈持续优化攻击策略；（2）防御侧：目标模型通过对抗性提示的微调训练提升鲁棒性，形成防御能力迭代增强。\n\nEvil geniuses: Delving into the safety of llm-based agents：\nEvil Geniuses框架，通过红蓝对抗演练自动生成针对大语言模型智能体的越狱提示。\n\n\n结合其他方法的基于LLM的攻击\nGoal-oriented prompt attack and safety evaluation for llms：\n提出将对抗性提示分解为三个核心要素：攻击目标、内容主体和模板框架。研究团队针对不同攻击目标人工构建了大量内容素材和模板变体。随后通过以下自动化流程生成混合提示：（1）组合生成：大语言模型生成器随机组合预定义的内容与模板，产生混合提示；（2）效果评估：大语言模型评估器对生成的混合提示进行有效性判定。\n\nTree of attacks: Jailbreaking black-box llms automatically：\n提出了一种名为剪枝攻击树（TAP）的新型越狱方法。该方法采用迭代优化机制：（1）种子提示生成：从初始种子提示出发，系统自动生成改进变体；（2）劣质提示剪枝：通过评估机制淘汰效果不佳的提示变体；（3）有效性验证：保留的优质提示输入目标大语言模型进行攻击效果验证；（4）迭代优化：成功实现越狱的提示将作为新一代种子提示进入下一轮优化循环。\n\n\n要点利用大语言模型模拟攻击者的方法主要包含两大策略：一方面通过训练LLM直接扮演人类攻击者的角色，另一方面构建多LLM协同框架，使不同模型作为独立代理协作自动化生成越狱提示。此外，LLMs还与其他攻击技术（如情景嵌套和遗传算法）结合，显著提升攻击成功率。\n4 防御方法\n4.1 提示词防御（Prompt-level Defenses)在无法直接访问模型权重和输出logits时，可以采用过滤函数来筛选或预处理输入的提示词。\n4.1.1 提示词检测（Prompt Detection）\n数据审核系统Llama-Guard2，对提示词和响应进行过滤\nTraining language models to follow instructions with human feedback：基于强化学习的微调\n\n但可以通过在恶意提示后附加不连贯的后缀以增加了型对提示的困惑度，进而绕过安全防护机制。Zou\n\n同时计算文字片段和整个提示词的困惑度进行阈值检测。Jain，LightGBM\n\n总结这些方法在防御GCG等白盒攻击时展现出良好的防护效果，但有较高误报率。\n4.1.2 提示词扰动（Prompt Perturbation）提示词检测可能带来高误报率，研究发现提示词扰动可以大大提高输入提示词的预测可信度。\n提示词转换并检查\nRA-LLM：对提示词叠加多种词级掩码，如果一定比例的这样的提示词复制被拒绝，则认为原输入恶意。\n\nSmoothLLM：对提示词叠加多次字符级扰动，最终选择能始终防御越狱攻击的提示词。Ji使用了相似的方法，不同之处在于其扰动方式是相同语义替换。\n\nJailGuard：对输入请求多次扰动观察输出的一致性，如果差异过大则认为本次为越狱请求。实现了图像和文本双模态的越狱检测。\n\nerase-and-check：删除提示词的某些token，检查相应的输出子串，如果任意子串被安全过滤器认为是有害的则提示词被认为恶意。\n\n\n防御前后缀\nZhou：提出了提示词优化算法来构造防御后缀，例如基于对抗提示词数据梯度下降优化后缀。\n\n总结提示词扰动方法通过利用提示中的细粒度内容（如词元级扰动和句子级扰动）来防御基于提示词的攻击，但一方面扰动可能降低原始提示的可读性，另一方面由于扰动在搜索空间中随机游走，难以稳定获得最优扰动结果。\n4.1.3 系统提示词防护（System Prompt Safeguard）\nSPML：一种领域专用的系统提示词框架，经历类型检查、中间表示转换等多个流程，最终生成鲁棒系统提示。\nSMEA：基于遗传算法首先以通用系统提示词作为初始种群，通过交叉重组与语义改写生成新个体，最终经过适应度评估筛选出优化后的提示种群。\nWang：将秘密提示词嵌入系统提示词，以防御基于微调的越狱攻击。由于用户无法访问系统提示词，该秘密提示词可作为后门触发器，确保模型始终生成安全响应。\nZheng：有害与无害的用户提示词在表征空间中呈现双簇分布，而安全提示词会使所有用户提示向量产生同向位移，从而导致模型倾向于生成拒绝响应。基于此发现，研究团队通过优化安全系统提示词，将有害与无害用户提示的表征分别导向不同方向，使模型对非对抗性提示作出更积极的响应，同时对对抗性提示保持更强的防御性。\n\n总结系统提示词防护机制提供了一种低成本的通用防御方案，能够适配多种攻击类型。然而当攻击者设计针对性攻击时，这类系统提示词仍可能被攻破。\n4.2 模型防御（Model-level Defenses)能修改模型权重时，模型防御利用了LLM自身的鲁棒。\n4.2.1 监督微调（SFT-based Methods）\nLlama2：高质量可信训练数据能提供良好的鲁棒性。\n\nBianchi：训练数据中加入安全数据（恶意指令和拒绝回复）会影响安全性，并且生成质量和安全性间需要权衡（过多的安全数据会使大模型过于敏感）。\n\nDeng：从对抗提示词中构建安全数据集，其首先利用LLM上下文学习能力进行攻击，然后迭代交互进行微调增强模型防御能力。\n\nBhardwaj：采用话语链(CoU)构建安全数据集进行微调。\n\n\n总结SFT训练的时间与经济成本相对可控，但该方法存在以下问题：灾难性遗忘的重大挑战；高质量安全指令集采集成本高昂；少量有害示例即可大幅提升越狱攻击成功率。\n4.2.2 基于人类反馈的强化学习（RLHF-based Methods）\nDPL：不完整数据的隐含背景（如标注者的背景信息）可能隐性损害偏好数据的质量。为此研究者提出将RLHF与分布偏好学习（DPL）相结合的方法，通过考量不同隐含背景因素，使微调后大语言模型的越狱风险显著降低。\n\nDPO：尽管RLHF复杂且往往不稳定但近期研究提出了直接偏好优化，也有一些其他工作使用DPO增强大语言模型的安全性（Gallego，Liu）。\n\n\n总结RLHF是提升模型安全性最广泛使用的方法之一，其优势在于：（1）经过RLHF训练的大语言模型在真实性方面显著提升，有害输出大幅减少，同时性能衰退微乎其微；（2）偏好数据的采集成本更低且更易获取。\n但该方法也存在明显缺陷：首先，RLHF训练过程耗时严重，由于奖励模型需基于生成结果计算得分，导致训练效率极低；其次，与SFT类似，其高昂的安全对齐措施容易被绕过。\n4.2.3 梯度与Logit分析（Gradient and Logit Analysis）防护者可以分析并操控梯度与Logit来检测潜在的越狱威胁并进行相应的防御。\n梯度分析基于梯度的分析防御从前向传播的梯度中提取信息作为分类特征。\n\nGradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis：\n比较关键安全参数与梯度之间的相似度，当超过阈值时认为是越狱攻击。\n\nGradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes：\n提出了”拒绝损失”的概念，用于衡量模型生成正常响应的可能性。他们发现，恶意提示与正常提示所获得的拒绝损失存在显著差异。基于这一发现，研究团队进一步开发了Gradient Cuff技术，通过计算梯度范数及拒绝损失的其他特征来识别越狱攻击。\n\n\nLogit分析基于logit的分析要开发新的解码算法来处理logit。\n\nSafedecoding: Defending against jailbreak attacks via safety-aware decoding：\n通过融合目标模型与安全对齐模型的输出logits，生成新的logits概率分布。在该分布中，有害token的概率密度被衰减，而良性token的概率密度则得到增强。\n\nRain: Your language models can align themselves without finetuning：\n在束搜索中引入了一种安全启发式机制：该机制通过评估单轮生成候选文本的有害性，并自动选择有害评分最低的候选输出。\n\n\n总结梯度与Logit分析方法无需更新模型权重，因而成为一种经济高效的检测手段。基于梯度的方法通过训练分类器来预测越狱行为，但分布外场景下的泛化能力存疑。此外，针对性对抗攻击可能劫持检测过程，导致分析失效。基于logit的方法则致力于开发新型解码算法以降低危害性，虽然成功率较高，但防御提示的可读性可能较差，且解码过程中的额外计算也会影响推理速度。\n4.2.4 自优化方法（Refinement Methods）利用LLM的自我改正的能力来降低生成恶意响应的风险。\n\nRLAIF：LLM知晓其在某对抗提示词下的输出可能不合适，因此可以迭代询问并修正回答。\n\nBreak the breakout: Reinventing lm defense against jailbreak attacks with self-refinement：\n验证了基础自优化方法在未对齐大语言模型上的有效性。他们建议将提示与响应格式化为JSON或代码结构，以此区分模型反馈内容。\n\nIntention analysis makes llms a good jailbreak defender：\n在自优化过程中设定明确目标以提升优化效果。具体而言，利用语言模型从伦理性和合法性等核心维度分析用户提示，并收集反映提示意图的模型中间响应。通过将这些附加信息嵌入提示，可显著提升模型生成安全准确响应的可靠性。\n\n\n总结尽管自优化方法无需额外微调流程，且在各类防御场景中表现优异，但其自我修正过程依赖模型内在纠错能力，可能导致性能不稳定。若大语言模型的安全对齐程度不足，基于自优化的防御机制可能失效。\n4.2.5 代理防御（Proxy Defense）使用其他模型进行安全检查。\n\nLlamaGuard：创新性地实现了双重内容分类，既对提示输入也对输出响应进行安全评估，可直接作为代理防御方案部署使用。\n\nAutoDefense：该多智能体防御框架由负责意图分析和提示词判定的智能体组成，通过协同检测有害响应并实施过滤，确保模型输出的安全性。\n\n\n总结代理防御方法不依赖于目标模型，并能有效抵御大多数基于提示的攻击。然而，外部检测器可能被逆向推导（Exploring the adversarial capabilities of large language models）。\n5 评测5.1 指标5.1.1 攻击成功率（Attack Success Rate）\n其中Ntotal为越狱提示词总数，Nsuccess为攻击成功的数目。\n安全评估器\n尚未有统一的结论定义什么是一次成功的越狱尝试（Jailbreakeval: An integrated toolkit for evaluating jailbreak attempts against large language models），主要有以下两种分类方式：\n\n基于规则：在LLM输出中检测关键词（Universal and transferable adversarial attacks on aligned language models，Is the system message really important to jailbreaks in large language models?）\n\n基于LLM：使用最新的大语言模型来评价攻击是否成功（Fine-tuning aligned language models compromises safety, even when users do not intend to!），可以得到二分结果或一个有害性分数。\n\n\n\n大部分基准使用基于大模型的评价方法，但评估过程各有不同：\n\nStrongReject：三维度评分：是否拒绝有害提示、生成内容是否精确匹配有害指令、输出结果是否符合现实逻辑。\n\nAttackEval：通过指令微调预训练大语言模型，进行三维度安全评估：目标模型是否成功拦截有害指令、生成内容是否精确匹配攻击意图、输出结果是否符合现实逻辑。\n\nJailbreakEval：创新性地实现基于投票机制的安全评估。作者的工作将当前主流的越狱成功判定方法系统归类为：人工标注、字符串匹配、对话补全和文本分类四大类（Jailbreakeval: An integrated toolkit for evaluating jailbreak attempts against large language models）。\n\n\n\n\n5.1.2 困惑度（Perplexity）\n困惑度用于衡量越狱提示词的可读性和流畅性。很多防御方法过滤高困惑度的提示词，因此低困惑度的越狱提示词更加值得关注。式中W&#x3D;(w1,w2,…,wn)，以token切分序列，Pr(wi |w&lt;i)为第i个token的输出概率。\n\nAutodan: Generating stealthy jailbreak prompts on aligned large language models\n\nAdvprompter: Fast adaptive adversarial prompting for llms\n\n\n5.2 数据集\n“Safety dimensions”指数据集中覆盖了多少种有害类别。\n\nTechHazardQA：要求模型以文字或伪代码给出答案来检测以特定格式输出时的模型表现。\n\nLatent Jailbreak：要求模型翻译可能包含恶意内容的文本。\n\nDo-not-Answer：全部是有害指令。\n\nXSTEST：包含安全与不安全指令来评估LLM在帮助能力和安全能力的平衡。\n\nSC-Safety：关注研究中文大模型，用多轮开放式对话进行测试。\n\nSafetyBench：设计覆盖了多类安全隐患的中英文多选问题。\n\nAdvBench：最初由GCG提出用于基于梯度的攻击。\n\nSafeBench：收集了能被转换为图像的有害提示词文本来攻击VLM。\n\nStrongREJECT：一个恶意问题的通用数据集。\n\nAttackEval：包含有基准真相的越狱提示词。\n\nHarmBench：特殊恶意行为，包含版权、上下文和多模态等等。\n\nSafety-Prompts：利用GPT-3.5-turbo加强的大量中文恶意提示词组成的数据集。\n\nJailbreakBench：覆盖OpenAI使用政策的混合数据集，每个恶意行为对应了正常行为。\n\nDoAnythingNow：一项基于网络平台提示的大规模调研，依据特征差异将其划分为不同社群类型。特别地，针对OpenAI使用政策禁止的敏感场景，运用GPT-4为不同社群生成定制化越狱提示，由此构建出涵盖各类禁忌问题的大规模数据集。\n\n\n5.3 工具包\nHarmBench：提供了一个红队评估框架，既能评估越狱攻击，又能评估防御方法。给定越狱攻击方法和目标模型，该框架用不同恶意行为尝试越狱该模型，并统一评估。\n\nSafety-Prompts：构建了一个专门针对中文大语言模型的安全评估平台，采用多场景安全测试框架：向目标模型输入不同安全等级的越狱提示，随后由大语言模型评估器对生成响应进行多维度分析，最终给出综合安全评分以判定目标模型的防御能力。\n\nJailbreakBench：本框架兼容越狱攻击与防御方法的双向测评，系统评估当前越狱研究的可复现性，集成了绝大多数前沿对抗提示、防御方法和评估分类器，并可通过模块化调用快速构建个性化评估流程。\n\nEasyJailbreak：提出了一套标准化的三阶段越狱攻击评估框架。在准备阶段，用户提供包含恶意问题和模板种子在内的越狱配置；在推理阶段，系统自动将模板应用于问题构建越狱提示，并对提示进行变异处理后再输入目标模型获取响应；最终在评估阶段，基于大语言模型或规则的评价器会对查询-响应对进行检测，生成整体安全指标。\n\n\n6 总结​\t本文系统构建了大语言模型越狱攻防方法的分类体系，研究发现：当前攻击方法正呈现效率提升与知识依赖降低的双重趋势，使得攻击更具实操性，这为防御研究提出了紧迫需求。\n​\t此外，本文通过横向对比现有评估基准，揭示了越狱攻防技术竞赛中的关键缺口，为后续研究提供切实启示。\n","categories":["论文阅读","大模型","越狱"]},{"title":"论文阅读——Don’t Say No: Jailbreaking LLM by Suppressing Refusal","url":"//posts/2505.004v1/","content":"论文概况题目：Don’t Say No: Jailbreaking LLM by Suppressing Refusal\n通讯作者：Wenjie Wang：wangwj1@shanghaitech.edu.cn\n作者院校：上海科技大学、中国科学技术大学、浙江大学\n发表于：arXiv\n论文内容1 研究背景与问题1.1 LLM安全对齐的挑战大型语言模型（LLMs）通过RLHF、模型微调等技术实现安全对齐，但仍面临越狱攻击（Jailbreaking）威胁，攻击者通过精心设计的输入诱导模型生成有害内容。这样的传统方法存在限制，现有方法（如GCG）通过优化对抗后缀最大化肯定响应（如“Sure, here is…”），但存在两大问题：\n\nToken Shift现象：损失函数平均计算所有token的损失，忽略前几个关键token的重要性。\n\n拒绝抑制不足：未显式抑制模型的拒绝响应（如“I cannot assist”）。\n\n\n1.2 现有评估方法的缺陷\n关键词匹配（Refusal Matching）：通过检测响应的前若干个长度是否不包含拒绝关键词（如“Sorry”）来判断攻击成功与否，但存在高误判率（见Table 2）。这是由于如果选定的检测长度过短，这个指标会忽视后续的拒绝内容，而过长则会将有害内容后面的拒绝内容检测到。\n\n\n\n\n\n2 核心方法：DSN攻击2.1 拒绝抑制（Suppress Refusal）\nUnlikelihood损失：降低模型生成预定义拒绝关键词的概率：$$\\mathcal{L}{Un}(p,q) &#x3D; -\\sum{i}p_ilog(1-q_i)$$\n$$\\mathcal{L}{\\text{refusal}}(x{1:n}) &#x3D; \\sum_{y \\in RKL} \\sum_{i&#x3D;n+1}^{n+H-RTL(y)} \\mathcal{L}{Un}(y,x{i:i+RTL(y)})$$其中，RKL为拒绝关键词列表（如“sorry, i cannot”, “unethical”），RTL为每个拒绝关键词的长度。该式的含义是在给定输入的条件下，对于拒绝关键词列表里的每个拒绝关键词，限定输出长度区间内的每一个RTL长度的窗口都要最小化Unlikelihood损失。\n\n\n2.2 肯定响应诱导（Elicit Affirmative Response）\n余弦衰减加权（Cosine Decay）：对生成序列的前几个token赋予更高权重，缓解Token Shift：$$CD(i) &#x3D; 0.5 + 0.5 \\cos\\left(\\frac{i}{H} \\cdot \\frac{\\pi}{2}\\right)$$其中i表示第i个token，H是序列长度。加权后生成目标响应的概率为：$$p_{CD}(x_{n+1:n+H}|x_{1:n}) &#x3D; \\prod_{i&#x3D;1}^H CD(i) \\cdot p(x_{n+i}|x_{1:n+i-1})$$\n\n肯定响应损失：$$\\mathcal{L}{\\text{affirmative}}(x{1:n}) &#x3D;-\\log p_{CD}(\\hat{x}{n+1:n+H}|x{1:n})$$该式的含义是在给定输入的条件下，最大化尽早生成指定输出的概率。\n\n\n2.3 综合损失函数$$\\mathcal{L}{DSN}(x{1:n}) &#x3D; \\mathcal{L}{\\text{affirmative}}(x{1:n})+ \\alpha \\cdot \\mathcal{L}{\\text{refusal}}(x{1:n})$$\n$$adv^*\\leftarrow argmin\\mathcal{L}{DSN}(x{1:n}\\oplus adv)$$\n目标为找到使综合损失最小的后缀adv*。\n3. 集成评估（Ensemble Evaluation）3.1 评估框架\n\n\n模块\n功能\n\n\n\nNLI矛盾检测\n使用自然语言推理模型检测响应中的语义矛盾，越大说明越低的回复连续性（算法1）\n\n\nHarmBench评估器\n基于微调的Llama-2分类器判断有害性\n\n\nGPT-4评估器\n通过提示工程判断生成内容是否符合攻击目标\n\n\n\n将输出划分为n个句子，计算每个句子与用户提问的一致性，以及句子之间的关系，然后根据句子长度加权得到总的矛盾得分，与预设阈值比较。\n3.2 评估效果对比\n\n\n评估方法\n准确率（%）\nAUC\nF1\n\n\n\nRefusal Matching\n74\n0.72\n0.79\n\n\nNLI\n80\n0.80\n0.81\n\n\n集成评估\n82\n0.79\n0.86\n\n\n\n4. 实验结果与分析4.1 攻击成功率（ASR）\n\n\n模型\nGCG（ASR%）\nDSN（ASR%）\n提升幅度\n\n\n\nLlama2-13B\n24\n38\n+58%\n\n\nVicuna-13B\n89\n95\n+7%\n\n\nMistral-7B\n92\n98\n+6%\n\n\n\n迁移性测试：DSN后缀迁移至GPT-3.5 Turbo的ASR达95%（GCG为34%）。\n\n4.2 消融实验\n余弦衰减的作用：移除后，Llama2-7B的ASR从38%降至22%。\n拒绝抑制系数α：α&#x3D;0.5时性能最优（见图3）。\n\n\n5. 贡献与讨论5.1 主要贡献\n理论创新：\n揭示Token Shift现象，提出余弦衰减加权。\n首次将拒绝抑制纳入越狱攻击目标函数。\n\n\n实用价值：\nDSN攻击在真实场景中仅需附加优化后缀（20 token），易于部署。\n集成评估减少误判率（F1提升7%）。\n\n\n\n5.2 局限性\n黑盒模型挑战：对GPT-4、Claude等高度对齐模型攻击仍困难。\n防御措施：PPL过滤可部分防御，但可通过添加无关前缀绕过（PPL从11k降至1.1k）。\n\n\n6. 总结与展望本文通过改进损失函数和评估方法，显著提升了越狱攻击的效果和评估可靠性。未来方向包括：\n\n将DSN扩展至多模态攻击\n探索动态拒绝关键词生成\n结合对抗训练提升模型鲁棒性\n\n复现代码修改记录\n由于环境的numpy版本过高，源代码中的np.infty全部被替换为np.inf\neval2_gpt.py中有笔误“whehter”，改为“whether”\neval2_gpt.py中的get_eval2_gpt4_results改为使用了硅基流动接口的deepseek-ai&#x2F;DeepSeek-V3模型\n\n\nTODO：\n改一下device，支持多卡\n","categories":["论文阅读","大模型","越狱"]},{"title":"学习资料汇总","url":"//posts/2501.001v1/","content":"介绍本部分为我个人学习DL相关知识时使用到的教程，在此进行记录。\n内容\n图学习\n入门：图神经网络GNN&#x2F;GCN教程\n\n\n\n","categories":["学习提升","学习资料"]},{"title":"动手学深度学习——线性神经网络","url":"//posts/2506.001v1/","content":"序言为系统性重温深度学习中的一些重要技术，深入掌握其底层原理及更高层次的思想，我选择使用《动手学深度学习》作为教材，并在此进行一些记录。\n线性神经网络线性回归\n线性回归基于几个简单的假设： 首先，假设自变量x和因变量y之间的关系是线性的， 即y可以表示为x中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。\n\n术语：\n\n训练数据集（training data set）、验证数据集（validation dataset）\n样本（sample）、数据点（data point）、数据样本（data instance）：每行数据\n标签（label）、目标（target）：试图预测的目标\n特征（feature）、协变量（covariate）：预测所依据的自变量\n权重（weight）\n偏置（bias）、偏移量（offset）、截距（intercept）\n超参数（hyperparameter）：可以调整但不在训练过程中更新的参数\n调参（hyperparameter tuning）：选择超参数的过程\n泛化（generalization）：找到一组能够在从未见过的数据上实现较低的损失的参数\n预测（prediction）、推断（inference）：给定特征估计目标的过程\n\n\n\n线性模型\n对于数据集：\n\n​\t线性回归的目标是找到一组权重向量w和偏置b： 当给定从X的同分布中取样的新样本特征时， 这组权重向量和偏置能够使得新样本预测标签的误差尽可能小。\n损失函数​\t损失函数（loss function）能够量化目标的实际值与预测值之间的差距。 \n\n平方误差：\n训练集n个样本上的损失均值：\n训练目标形式化定义：\n\n梯度下降每次加载全部数据集过于缓慢，因此一般采用小批量随机梯度下降\n\n\n初始化模型参数的值后，反复抽取样本并在负梯度的方向上更新参数，对于平方损失和仿射变换：\n\n\n正态分布\n均方损失可用于线性回归的一个原因是假设了观测中包含噪声且噪声服从正态分布：\n\n\n通过给定的x观测到特定y的似然（likelihood）：\n\n\n根据极大似然估计法，参数w和b的最优值是使整个数据集的似然最大的值：\n\n\n即最小化负对数似然：\n\n\n\n\nTO BE CONTINUED\n","categories":["学习提升","深度学习"]},{"title":"论文阅读——Harnessing explanations Llm-to-lm interpreter for enhanced text-attributed graph representation learning","url":"//posts/2506.002v1/","content":"论文概况题目：Harnessing explanations Llm-to-lm interpreter for enhanced text-attributed graph representation learning\n通讯作者：Bryan Hooi：bhooi@comp.nus.edu.sg\n作者院校：新加坡国立大学、洛约拉马利蒙特大学、纽约大学、Meta AI\n发表于：ICLR 2024\n代码仓库：https://github.com/XiaoxinHe/TAPE\n论文内容引言​\t文本属性图（TAGs）广泛存在（如论文引用网络），但现有方法存在局限，本文核心创新为提出LLM生成的解释作为特征：\n\n通过提示LLM输出预测标签和决策解释，提取其知识与推理能力。\n设计 LLM-to-LM解释器，将文本解释转化为GNN可用的向量特征。\n\n相关工作\n浅层特征+GNN：使用skip-gram等算法提取文本浅层特征，然后用GCN等图学习算法，但问题在于浅层特征语义捕捉能力弱。\n小型LM微调+GNN：使用BERT等语言模型进行特征编码，然后再使用GNN，但问题在于计算成本高，且缺乏复杂推理能力。\nLLM+图结构理解：虽然已有工作研究LLM对于图结构的理解，但未针对TAG任务优化。\n\n定义\n文本属性图\n\n\nLM特征提取\n\n\nLLM\n\n\n图神经网络\n\n\n\n论文方法\n\n对于每一个论文，将摘要、标题、问题输入大模型，要求大模型预测的分类以及相应的解释。\n\n微调LM来从LLM生成的预测和解释中提取特征用于后续的GNN。\n\n\n\n大模型生成的预测使用独热编码，并线性变换拼接到特征向量中\n\n分别利用orig（原始文本）、expl（LLM生成的解释）、pred（LLM生成的预测）特征来训练GNN模型，平均后作为预测结果\n\n\n结果\n复现1 LLM Direct\n","categories":["论文阅读","大模型","图学习"]},{"title":"雅思学习——L0 雅思真经第一课","url":"//posts/2507.001v1/","content":"Listening\n考点特色：\n（Sec1）生活场景：旅游（机场、火车站、景点）、咨询（住房、工作、俱乐部）、银行、医疗\n（Sec3）学习场景：图书馆、论文作业、考前复习\n\n学习方法：\n精听跟读：做题——对着听力原文放音，记录没听出来的单词，重复读\n\nReading题型：3篇文章40道题60分钟\n单词、句子、段落、匹配\nWriting题型：\nTask 1：150词 线表饼柱图表说明\nTask 2：250词 议论文、说明文\n\n评分标准：\nTR：回应任务\nCC：逻辑\nLR：词汇水平\nGRA：语法范围和准确度\n\n学习方法：\n逻辑框架模板\n\n准确通顺，言之有物\n\n\nSpeaking题型：\nPart 1：5分钟 一般陌生人见面聊天场景\nPart 2：1+2分钟 根据提示卡片上的问题引导思考一分钟后独白\nPart 3：5分钟 根据话题深入互动对话\n\n学习方法：背诵输入，句型为重，自然放松，逻辑沟通 \n","categories":["雅思学习"]},{"title":"论文阅读——PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models","url":"//posts/2507.002v1/","content":"论文概况题目：PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models\n通讯作者：Aymen Shabou：aymen.shabou@credit-agricole-sa.fr\n作者院校：Ecole Polytechnique，Universite Sorbonne Paris Nord，Credit Agricole S.A\n发表于：CVPR 2025\n代码仓库：PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models\narxiv版论文：arxiv.org&#x2F;pdf&#x2F;2504.08966\n论文内容摘要视觉语言模型由于需要额外输入token来表示视觉信息，在推理阶段需消耗大量计算资源。然而这些视觉token常包含冗余且次要的内容，导致token数量不必要地膨胀。\n为解决该问题，我们提出PACT方法，通过在语言模型的早期层修剪无关token并合并视觉冗余token，显著降低推理时间与内存占用。我们的方案采用新型重要性度量指标识别无关token（不依赖注意力分数机制），确保与FlashAttention兼容；同时提出名为”距离有界的密度峰值聚类”的创新算法，该算法在预定义距离阈值的约束下，可高效聚类视觉token。通过大量实验，我们验证了PACT的有效性。\n方法\nPACT包含三个实施步骤：首先，定位无关紧要的token并删除；随后，对筛选后的token进行聚类；最终，将各聚类簇中的token与距离阈值范围内先前被丢弃的token重新融合。\nPACT在语言模型的选定层L中运行，适用于将视觉token输入语言模型的场景，且不受视觉编码器或连接器架构的限制。\n1 删除低重要性token\ntoken重要性定义旧方法：该标记从所有其他标记接收到的总注意力分数。\n\n问题1：现有VLM使用 FlashAttention，不支持输出注意力分数；\n问题2：注意力分数计算时有掩码，引入前后位置的偏见，靠后的token倾向于收到更少的注意力。\n问题3：因为每个自注意力层会聚焦于视觉标记的不同特征维度，所以仅依靠单一层的Q、K来确定重要性指标可能无法全面捕捉显著性\n\n\nEfficient Unimportant Tokens Identification (EUTI)：利用隐藏层积累的信息与特定层的Q、K信息来判断。\n\nStep1——计算全局query：该向量表征了视觉标记在层L上通过所有注意力头请求的全局查询信息。\n\n\nStep2——计算每个视觉标记的重要性分数：首先计算其键向量与全局query的点积，然后在每个注意力头内对视觉标记进行softmax归一化，最后跨注意力头取平均值。最终分数通过将结果与隐藏状态范数相乘得到。\n\n\nStep3——控制不重要标记的比例：设定参数λ∈[0,1]将视觉标记划分为重要标记与不重要标记两类。\n\n\n\n2 聚类重要token\nDistance Bounded Density Peaks Clustering (DBDPC)：\n本聚类算法特点为计算时间少，并避免将特征不相似的点聚到同一类\n保证每个向量到其聚类中心的距离均小于d_c，簇间距离上限为2d_c×(2−d_c)\n使用注意力机制里的K来计算\n\n\n\n3 重新融合token\n将距离簇中心点足够近的被删除的点重新加回簇内：\n\n\n合并各个簇内的隐藏层状态：\n\n\n更新新隐藏状态H’的位置ID：\n为保持与常规推理过程的低统计差异度，将H′中每个向量的位置ID设为其对应聚类中心的ID。\n\n比例注意力：为防止合并词元降低影响力，采用比例注意力机制利用各词元的权重让模型能有效将每个视觉词元视为多个词元的集合。其中，矩阵W表示各词元的权重，B为注意力掩码。\n\n\n实验\n复现\n研究lmms_eval.models.llava_onevision.py的generate_until函数（386行）\n\n研究ConfigurableTask.doc_to_visual\n\n在lmms_eval.models.llava_onevision.py的process_images（458行）将PIL图像visual转换为tensor\n\n\n","categories":["论文阅读","大模型","多模态"]},{"title":"论文阅读（综述）——Graph foundation models: Concepts, opportunities and challenges","url":"//posts/2507.003v1/","content":"论文概况题目：Graph foundation models: Concepts, opportunities and challenges\n通讯作者：Chuan Shi： shichuan@bupt.edu.cn\n作者院校：北京邮电大学，新加坡管理大学，伊利诺伊大学芝加哥分校等\n发表于：TPAMI 2025\n代码仓库：无\n论文内容摘要基础模型已成为各类人工智能应用中的核心组件，在自然语言处理及多个其他领域展现出显著成效。与此同时，图机器学习领域正在经历从浅层方法向复杂深度学习方法的范式转变。基础模型强大的泛化与适应能力，促使图机器学习研究者开始探讨发展新型图学习范式的可能性——该范式旨在通过海量图数据预训练出可适配多种图任务的通用模型。尽管这一新兴领域引发了广泛关注，但目前仍缺乏明确的定义界定与系统化的分析框架。\n为此，本文首次提出图基础模型（Graph Foundation Models, GFMs）的概念体系，并对其核心特征与支撑技术进行了全面阐释。我们依据模型对图神经网络与大语言技术的依赖程度，将现有研究成果划分为三大类型。除系统梳理GFMs研究现状外，本文还前瞻性地探讨了这一快速发展领域未来可能的研究路径。\n介绍\n基础模型具涌现性和通用性等特征，当下的基础模型能处理文本、图像、视频、音频等多模态输入。\n\n图机器学习从随机游走、矩阵分解等浅层方法向深度学习转变，例如GNN引入消息传递机制在节点分类、链接预测、图分类和图聚类等任务中取得了显著成效，但在表达能力和泛化性方面仍有局限。\n\n引出了对图基础模型（GFM）的研究，来实现在图领域的涌现性和泛化性。\n\n\n\n背景图深度学习图数据特性核心挑战源于：（1）其数据的非欧几里得性，在规模和形态上存在极大变异性。（2）不同领域的图数据具有不同的节点类型和边语义。（3）图数据包括同构图、异构图、超图和动态图等多种类型。\n主干结构GNN是主流结构，大多数遵循消息传递框架。例如，图卷积网络GCN（Semi-supervised classification with graph convolutional networks），采用归纳学习的GraphSAGE（Inductiverepresentationlearning on large graphs），图注意力网络GAT（Graphattention networks）。\n但GNN深度增加会导致节点表征趋同以及信息过度压缩，改进方法包括DropEdge（Dropedge:Towardsdeepgraph convolutional networks on node classification）、图Transformer模型（Do transformers really perform badly for graph rep resentation?、Structure-awaretransformerfor graph representation learning、Rethinking graph transformers with spectral attention）等。\n学习范式\n监督学习：利用带有输入数据和输出标签的训练数据集应用于图分类和图回归等问题，如分子属性预测。\n半监督学习：同时利用标记和未标记数据提升模型性能，如节点分类。\n无监督学习：图聚类通过节点关系识别结构，链接预测推断缺失连接。\n\n语言基础模型语言数据特性具结构化，更易建模；有知识迁移性，更易建立通用表征。\n主干结构预训练语言模型——大语言模型（扩大模型参数量和训练数据量）\n学习范式利用大规模标注数据集和无标注文本数据，执行（1）预训练-微调：首先作为语言模型学习预测文本数据的概率分布并通过微调使模型适配特定任务；（2）预训练-提示-预测：通过文本提示重构下游任务形式。\n图基础模型\nGFM定义：在大量图数据上预训练，并能适用于一系列下游图任务。\n\n证明预训练（pre-training）加适应（adaptation）效果优于图深度学习：\n\nGraphprompt: Unifying pre training and downstream tasks for graph neural networks\nAll in one: Multi-task prompting for graph neural networks\n\n\n涌现性：体现在语境内学习、图推理、图生成等任务。但相关研究较少（如PRODIGY: Enabling in-context learning over graphs）。\n\n通用性：体现在模型通用于多种任务，如节点分类、连接预测、图分类等，但难点在于如何协调表达各任务\n\n预训练：\n\n对比自监督学习（正负样本）：Deep graph contrastive representation learning\n生成自监督学习（结构重建与预测）：Graphmae2:Adecoding-enhancedmaskedself-supervised graph learner\n\n\n适应：\n\n普通微调(Vanilla FT)：在特定任务数据上训练整个预训练模型\n参数高效微调(Parameter-efficient FT）：调整模型参数的一个子集\n\n\nGFM与LLM差异：语言模型专为处理欧几里得数据（文本）设计，而图模型则面向非欧几里得数据（图结构）或混合数据（如图属性），能捕捉更复杂的关联关系，但数据稀疏性显著，缺乏统一表征基础，图结构还可能呈现层次性、循环性等异质特征。\n\n\n类别一：GNN-BASED MODELSA. 主干结构基于消息传递的方法（MPNNs）：每个节点从邻居节点聚合信息，处理后继续传递，形如：\n可以理解为：每个节点依靠上一层的本节点特征信息与各邻居节点和边的信息来更新。\n\nGCN（图卷积网络）：通过局部一阶近似谱图卷积捕获图结构特征与编码节点属性。\nGAT（图注意力网络）：采用注意力机制驱动的加权聚合策略。\nGraphSAGE：采样固定规模的邻域节点子集，聚合处理这些采样邻居的嵌入表示进行学习。\nHGT（异构图 transformer）：采用类型特异性参数来定义图中各边上的异构注意力。\nGIN（图同构网络）：一种理论表达能力与1-WL图同构等价的基于消息传递的模型。\n\n基于消息传递的图神经网络更详细的综述：2003.08271，2105.07342，ieeexplore.ieee.org&#x2F;ielaam&#x2F;34&#x2F;10008914&#x2F;9764632-aam.pdf\n基于图transformer的方法：GNN会遇到表达力有限、过平滑、过压缩等问题，因此图transformer受到关注，其利用注意力机制处理整张图。\n\nGraphBERT：采用基于亲密度和跳数的相对位置编码来表示子图中节点的位置信息。\n\nGROVER：采用定向消息传递网络捕捉分子图的方向特性并区分不同类型的边。\n\nGraphormer：通过空间编码表征节点关系，将最短路径距离作为偏置项引入注意力机制。\n\n\n关于图transformer更详细的综述：Attending to Graph Transformers | OpenReview\nB. 预训练利用大量未标注的节点和图的数据进行自监督预训练。\n基于对比学习的预训练方法：对比式图预训练方法旨在最大化不同视图（局部、上下文或全局视角）间的互信息，使模型学习跨视图不变的语义特征。\n\n同尺度对比学习：对相同层级的图视图进行对比，如GCC将节点的子图嵌入作为表征，将同一节点的不同子图视为正样本、不同节点的子图作为负样本，通过噪声对比估计(NCE)损失实现正样本对齐与负样本分离，从而捕捉通用模式。其他方法还有GraphCL、GRACE、MA-GCL、GCOPE、FUG等\n跨尺度对比学习：对比不同层级的图视图，如DGI利用最大化节点嵌入与全图嵌入的互信息，同时最小化节点与扰动图嵌入的信息量，促使编码器捕获图的全局信息，但会忽略不同节点间的差异性特征。\n\n基于生成的预训练方法：旨在使图神经网络理解图数据的通用结构与属性语义，从而使其能够基于通用信息适配下游任务。但生成式方法的准确性和合理性仍需提升。\n\n图重构方法：重建给定图的特定部分。如VGAE采用GCN作为编码器生成节点嵌入，然后通过节点嵌入的内积重构邻接矩阵。其他方法还有GPT-GNN、GraphMAE等。\n属性预测方法：学习并预测图的深层特性。如GROVER要求模型预测局部子图中的上下文相关属性，将基元预测建模为多标签分类问题。\n\nC. 适应预训练所使用的任务一般与下游任务不一致，因此需要微调技术来使模型适应新任务。尽管微调方法已取得显著成效，但通常需要大量标注数据来调整模型参数，计算开销大。\n微调：利用预训练模型生成节点嵌入或图嵌入，随后微调外部任务特定层，使预训练模型能够泛化至下游任务。\n\nDGI和GRACE采用预训练编码器获取节点嵌入，再通过标注数据微调逻辑回归分类器以处理节点分类任务。\n\nGPT-GNN利用标注数据微调下游任务特定解码器，引导预训练模型适配下游任务。\n\nAdapterGNN在消息传递阶段前后设置并行适配器来修改输入图结构，此方法仅需微调新增参数。\n\nG-Adapter使用面向图变换器的参数高效微调方法，通过消息传递将图结构融入微调过程。\n\nG-TUNING使用基于图重构的GNN微调策略，保持生成模式并解决预训练与下游数据集间的结构差异。\n\n\n提示词调优：此方法避免全参数调整，在促进多任务适应与零样本学习方面展现出优势。\n\n前提示方法：通过改造输入图的拓扑结构或节点特征来辅助下游任务，或构建提示图增强模型适应性。例如AAGOD使用以数据为中心的操作方法，通过在原始输入图的邻接矩阵上叠加可学习的提示放大器。其他方法还有All In One、GPF、PRODIGY、IGAP、TPP等\n\n后提示方法：在消息传递后的表征上应用任务特定提示。例如GPPT采用提示函数生成每个类别的标记对，将节点分类任务转化为链接预测。其他方法还有GraphPrompt、GraphPrompt+、ProNoG等。\n\n\n总结基于图神经网络的模型能有效处理图结构数据、训练成本低、资源利用率高，通过图中标签信息的传播，在标注数据稀缺时仍保持较强泛化能力。针对异质图（CPT-HG）、超图（PhyGCN）、时序图（GraphST）等复杂图数据也有相应研究。\n但这类模型文本建模能力薄弱，难以充分挖掘节点&#x2F;边关联文本属性的丰富语义信息，且通用知识整合能力受限，在需要跨域泛化或常识推理的任务中表现受限。\n类别二：LLM-BASED MODELS将LLM作为主干有以下显著优势：在图数据中有效融合文本信息；利用自然语言处理多种图学习任务；实现图推理。遇到的核心问题在于如何实现图数据与自然语言的对齐，以使LLM能够理解图结构。\nA. 主干结构由于LLM最初以词元（token）作为输入，要实现图结构信息的细粒度建模较难，主要含图到词元和图到文本两种方法，其区别在于是否使用额外编码器（图到词元方法需要借助编码器为每个节点生成嵌入级表示）。\n图到词元（graph-to-token）：将图数据序列化为词元，并解决图结构信息的编码问题，一般使用开源大语言模型作为主干模型。\n\n**GIMLET：**结合广义位置嵌入和基于指令的预训练，使大语言模型能同时处理图与文本数据。\n**Meta-Transformer：**提出了支持图数据、文本和图像等多模态数据的Transformer架构。\n**InstructGLM：**采用预训练-适应框架，引入大语言模型增强文本处理能力。将图中固有的节点特征向量作为独特词元扩展至大语言模型的词表。\n\n图到文本（graph-to-text）：采用自然语言描述图信息，可使用任何大语言模型作为主干。但当前阶段的提示词使用方法难以有效挖掘图数据的底层结构特征。\n\n**NLGraph：**系统评估了大语言模型在八种图推理任务中的表现，并测试了自然语言形式下的经典图神经网络任务。基于边列表描述方法，印证了该方式在处理复杂图问题时的局限性。\n\n**TextForGraph：**设计了完整文本与精简文本两种提示词格式描述图信息，压缩了提示长度。\n\n**When&amp;Why：**使用多风格提示词设计提供了结构化数据处理方法。\n\n**GraphWiz：**针对环路检测、子图匹配等不同图任务定制了专属提示词方案。\n\n**GPT4Graph：**创新性地提出混合提示工程方法，将人工构建提示词（边列表、邻接表等）与模型自生成提示词（图摘要、邻域汇总等）相结合。研究证实，自生成提示能更有效帮助大语言模型理解图结构。\n\n**Graph-LLM：**进一步支持GPT4Graph的结论，指出邻域汇总是现有提示词工程中最有效的技术。\n\n\nB. 预训练基于大语言模型的图学习方法主要采用LM与MLM。\n语言建模（LM）：本质上可归结为对下一个词概率分布的预测问题，通过在大规模语料上采用最大似然估计（MLE）训练网络，可有效学习这些概率。然而单向语言模型的缺陷在于上下文信息仅依赖于左侧上文及词元本身，若要获得更具鲁棒性的文本上下文表征，则需要同时捕获前向与后向的上下文信息。\n掩码语言建模（MLM）：随机遮蔽输入句子中的特定词元，要求模型通过分析上下文预测被遮蔽内容，该任务常被称作完形填空任务。MLM存在预训练-微调阶段割裂的问题——由于微调阶段不出现掩码标记，导致两阶段目标不一致。\nC. 适应无论是图到词元还是图到文本方法，都配备了特定的适应技术以增强大语言模型对图数据的理解能力。从提示词工程的角度将这些适应策略分为两类：人工型与自动型。\n人工提示型：采用人工设计的前缀式提示模板。\n\n**LLMtoGraph、NLGraph：**整合节点列表、边列表及其他自然语言描述的图属性，构建复合型提示模板。\n\n**GPT4Graph：**采用边列表、邻接表等多种描述语言表示图数据。\n\n**InstructGLM：**创新性地采用指令式提示设计以中心节点为核心的图描述集，并结合任务专属描述。\n\n\n自动提示型：采用大语言模型自动生成的提示模板进行适应性优化。\n\n**GPT4Graph：**采用图摘要（提取关键特征或目标节点邻域信息生成图结构概要）、图探索（自动生成查询序列以检索图信息）和图补全（构建部分图结构后引导模型完成缺失部分）这三种自生成提示。\n\n**Graph-LLM：**采用邻域摘要形式的自动提示。\n\n\nD.讨论\n除提示工程外，还存在多种基于微调的适应方法，包括常规微调（Vanilla Fine-Tuning）、中间层微调（IFT）、多任务微调（MTFT）以及参数高效微调（Parameter Efficient Fine-Tuning）。尽管这些方法尚未应用于图任务，但它们为预训练模型的下游适配提供了有效途径。我们预期未来研究将探索这些适应方法与图任务的结合，进一步推动图基础模型的发展。\n\n当前将LLM作为图学习主干的方法存在固有局限：1）难以有效处理描述图结构所需的长文本信息；2）无法通过图链接实现多跳逻辑推理；3）对高连通图的拓扑结构捕捉能力不足；4）难以适应随时间演化的动态图特性。\n\n图到文本方法受限于LLM的输入长度，而图到词元方法虽能通过单节点单词元映射处理大规模图数据，却需承担更高计算成本。\n\n未来研究方向应包括：1）增强LLM对节点特征与拓扑结构等图关键信息的理解效率；2）开发结构化图建模技术，弥补自然语言描述与图数据完整信息间的语义鸿沟；3）拓展应用场景，如LLM4DYG已探索时态图应用，但超图和异构图等复杂图类型仍有待开发。\n\n\n类别三：GNN+LLM-BASED MODELSGNN缺乏文本处理能力，LLM无法执行精确数学运算、难以处理多跳逻辑推理，将两者进行整合有望开发出更全面、更强大的模型。\nA. 主干结构\n以图神经网络为核心的方法：利用LLM从原始数据中提取节点特征，并通过GNN进行预测。\n\n**GraD：**使用PEFT在TAG数据集上微调，移除头部层后获得微调后的节点表征，继而训练GNN。\n\n**TAPE：**针对ChatGPT等无法直接获取嵌入的LLM，通过文本交互生成排序预测列表与解释，再微调语言模型将原始文本与LLM生成的预测特征转化为节点特征供下游GNN使用。\n\n**GIANT：**采用图结构感知的自监督学习方法微调语言模型，使文本表征包含图结构信息。\n\n**WTGIA：**专注于文本级图注入攻击，提升攻击的可解释性与实际应用性。\n\nGALM： 研究文本与图数据的联合预训练方法，特别针对富含文本的大规模异质图。\n\n**OFA：**提出用自然语言描述节点&#x2F;边的文本属性图，通过语言模型统一至共同嵌入空间。\n\n**Heterformer：**在Transformer层中同步编码节点文本与异质结构信息。\n\nEdgeformers： 基于图增强Transformer，通过边关联文本的上下文建模进行边&#x2F;节点表征学习。\n\nLLMRec： 采用三种LLM图增强技术改进推荐系统，解决隐式反馈稀疏与辅助信息低质问题。\n\n**WalkLM：**通过属性随机游走生成近似有意义的文本序列，微调语言模型后提取同时捕获属性语义与图结构的嵌入向量。\n\nTOUCHUP-G： 增强预训练模型的节点特征用于下游图任务，但现有多路GNN的节点属性初始化方法难以完整捕获关联文本语义。\n\n**METERN：**使用单一文本编码器建模关系间共享知识，辅以少量关系特定参数生成定制化表征。\n\nLLM-GNN： 构建无标签流程，利用LLM生成标注并为GNN提供训练信号。\n\n\n对称式方法：通过对齐GNN与LLM的嵌入空间以优化预测或下游任务性能，对称式方法通过协同机制获取结构感知的文本特征。\n\n**GraphFormer：**将文本嵌入与图聚合融合为迭代流程，相连节点会在分层GNN组件中进行信息交换，使各节点融合邻域信息。但该方法存在可扩展性问题。\n\n**GLEM：**采用变分EM框架交替更新LLM与GNN，LLM捕捉局部文本属性的节点标签分布，GNN预测表征全局条件标签分布，缓解可扩展性问题。\n\n**G2P2：**基于三种图交互对比策略预训练图-文本联合模型，进而探索下游任务的提示学习。\n\n**ENGINE：**通过可调节侧边结构整合LLM与GNN，显著降低训练复杂度同时保持模型能力。\n\n**PATTON：**提出两种预训练策略：网络语境化掩码语言建模与掩码节点预测，以捕获文本属性与网络结构的固有关联。\n\n**OpenGraph：**开发灵活的基础图模型，通过理解异构图数据的复杂拓扑模式，在零样本图学习任务中表现优异。\n\n**RLMRec：**通过语义空间对齐与协同关系建模，结合LLM提升推荐系统的表征学习能力。\n\n\n以LLM为核心的方法：\n**GraphTranslator：**采用图模型高效处理预定义任务，并利用LLM的扩展接口支持图模型的开放式任务。\n**GraphGPT：**通过图指令微调将图结构知识注入LLM，使其理解复杂图结构并提升跨数据集与任务的适应性。\n**THLM：**提出融合文本属性异构图拓扑与异构信息的预训练框架，显式增强语言模型的图感知能力。\n**GraphPrompter：**通过软提示实现图信息与LLM的对齐。\n**InstructGraph：**结合指令微调与偏好对齐，赋予LLM图推理与生成能力。\n**TEA-GLM：**先通过对比学习预训练GNN捕获图结构与语义信息，再经线性投影器将GNN表征转化为统一的任务指令输入LLM，实现无需微调的跨数据集与跨任务泛化。\n**G-Retriever：**提出面向现实文本图的检索增强生成框架，通过对话式接口实现问答功能，有效缓解幻觉问题并支持大规模图的高效扩展。\n\nB. 预训练GNN加LLM的方法可以同时在文本数据和图数据上进行训练，分为基于GNN或LLM的方法和基于对齐的方法。\n\n**SimTeG：**融合了文本-文本对比学习（TTCL）技术，利用了预训练阶段某些文本对比随机选取的文本对具有更高语义相似性的特性。\n\n**GALM：**在大规模图数据集上进行图重构预训练，从而将图结构信息有效整合到预训练语言模型中。\n\n\nC. 适应除少数研究在零样本任务上测试模型性能外，大多数情况下模型都需要进行适配。适配策略分为两大类：微调与提示调优。\n微调方法：常规微调需要调整大量模型参数，存在计算密集和资源消耗大的问题。参数高效微调方法则实现了更高效节能的下游任务适配。例如利用分子图-文本配对数据对齐GNN与LLM的嵌入空间，针对TAGs进行分类任务调优，通过生成文本标注或描述来适配下游任务。\n提示调优方法：\n**G2P2：**通过提示调优自动优化提示模板，仅需少量标注数据即可高效适配下游任务。\n\n**TAPE：**充分利用语言模型的内生能力，无需额外微调或参数调整，仅依赖模型预训练知识即可生成文本输出。\n\n\nD.讨论\n将LLM与GNN对齐到统一表征空间仍具挑战性，为解决这一问题，需建立衡量两者表征对齐程度的标准。\n\n现有研究已开始将GNN+LLM方法拓展至异质图与超图领域，如HiGPT提出了情境感知的异质图标记器与异质性感知指令微调框架，GHGRL利用LLM自动归纳和分类异质图数据的多格式多类型数据，Hyper-BERT添加超图感知层来增强预训练BERT模型用于节点分类任务。\n\n\n挑战与展望A.  数据与评估面临的挑战数据评估与质量：数据规模与质量的提升是基础模型效能提升的关键因素，而当前开源大规模图数据仍较为有限，各数据集多集中于单一领域，且有噪声、不完整或未经妥善处理的数据将影响图基础模型的性能。研究者已从图结构学习、特征补全、标签混合等多角度提出数据增强策略。然而现有数据增强技术多针对单一GNN模型设计，如何面向基于LLM或”GNN+LLM”架构的模型进行有效图数据增强仍需探索。\n评估体系：开放式任务缺乏标准标签，如何评估图基础模型在开放式任务中的性能成为难题。在语言基础模型领域，对开放式任务的评估已从人工评估发展到元评估，现有LLM评估方法是否适用于图基础模型仍有待验证。此外，还需对图基础模型的鲁棒性、可信度及综合性能进行系统评估。\nB. 模型相关挑战模型架构：\n在骨干架构方面，近期研究提出的超越Transformer的架构已展现出更优性能或可解释性，但这些架构能否处理图数据仍是未知数。\n\n在GNN+LLM联合模型中，如何更有效地对齐二者输出值得探索。\n\n面对异质图、时序图、超图等多元图结构，设计能处理多类图数据的GFM是重要研究方向，例如使用专家混合模型。\n\n探索如何利用GNN扩展多模态基础模型的模态覆盖范围或增强多模态学习能力是颇具价值的研究方向。\n\n\n模型训练：\n设计合适的预训练任务至关重要，针对不同GFM架构已衍生出多样化的预训练任务形式。各类预训练任务是否存在适用边界、未来是否会出现统一范式都值得深入研究。\n\n如何使图基础模型支持跨域数据仍待研究。现有研究或采用多领域数据作为预训练输入，或通过LLM嵌入、条件生成和零样本迁移等方法实现跨域适应。除本文涉及的微调与提示学习外，知识蒸馏、人类反馈强化学习和模型编辑等技术在提升效率或更新知识方面具有潜力。\n\n\nC.应用层面挑战杀手级应用：\n图基础模型能否在图任务中催生突破性应用尚未可知，对于适合图神经网络应用的场景潜在研究方向包括：结合LLMs的图模型以更好支持开放式任务，或通过图学习技术增强LLMs的推理能力。\n\n传统交通预测技术多集中于出行需求预测和交通流量预测等单一任务，缺乏对交通系统的整体认知。将交通系统视为时空图时，图基础模型可捕捉参与者的行为模式，从而为城市计算问题提供统一解决方案。\n\n\n可信度问题：\nLLM的黑箱特性引发了幻觉输出和隐私泄露等安全隐患，近期工作指出，预训练GNNs同样存在公平性和抗攻击鲁棒性方面的可信风险。鉴于图数据的特殊性，需采用置信度校准或反事实推理等技术防范GFMs的安全风险。此外，GNN和LLM均存在的隐私风险使得GFMs的隐私增强成为关键议题，联邦学习、RLHF和红队测试等方案的应用可行性尚待验证。\n现实场景中的图数据常面临噪声、类别不平衡、数据残缺和多模态特征等挑战，如何利用非常规图数据构建GFMs或适配现有模型，将成为未来研究的重点方向。\n\n","categories":["论文阅读","大模型","图学习"]}]