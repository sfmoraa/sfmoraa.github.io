[{"title":"使用LoRA微调Llama-2-7b-hf实现涉诈短信识别","url":"//posts/2025/05/blog1/","content":"本博客为2024挑战杯项目基于大模型的多模态风险内容识别系统的涉诈短信识别功能的实现。\n方案选择Huggingface格式LLama模型+Lora代码微调\n环境准备GPU服务器：RTX 4090，24G双GPU，cuda12\nPython: 3.11\n由于40系GPU不支持某些高效的通信模式，需要设置环境变量：\nexport NCCL_P2P_DISABLE=1export NCCL_IB_DISABLE=1\n\n\n\n模型准备模型下载下载Llama-2-7b-hf模型，使用的是Llama中文社区整理的模型资源。\nLlamaFamily&#x2F;Llama-Chinese: Llama中文社区，实时汇总最新Llama学习资料，构建最好的中文Llama大模型开源生态，完全开源可商用\n模型验证可以用以下代码测试下载的模型的效果，注意修改模型保存的路径，此处为&#x2F;home&#x2F;data&#x2F;pre_model&#x2F;Llama-2-7b-hf。\nfrom transformers import AutoTokenizer, AutoModelForCausalLMimport torchmodel_path = &quot;/home/data/pre_model/Llama-2-7b-hf&quot;tokenizer = AutoTokenizer.from_pretrained(model_path)model = AutoModelForCausalLM.from_pretrained(    model_path,    device_map=&quot;auto&quot;,        # 自动分配GPU资源).eval()                      # 启用评估模式提升推理速度input_text = &quot;How to learn skiing?&quot; inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;).to(model.device)with torch.inference_mode():      outputs = model.generate(        **inputs,        max_length=256,        do_sample=True,       # 启用采样生成更自然文本        temperature=0.7,              top_p=0.9                 )print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\n输出如下，可以看出生成的文本比较流畅。\nHow to learn skiing?Skiing is an exciting and fun winter activity that many people love. While skiing can be challenging at first, with the right instruction and practice, anyone can learn how to ski.Learning to ski is a process that requires patience and practice. It is important to start with the basics, such as learning how to balance on skis, and progress gradually to more advanced techniques.The best way to learn how to ski is to take lessons from a qualified instructor. A qualified instructor will be able to teach you the basics of skiing, such as balance, turning, and stopping. They will also be able to teach you more advanced techniques, such as carving and jumping.Another way to learn how to ski is to practice on a ski slope. Ski slopes are designed to help you learn how to ski safely and effectively. They are usually divided into different levels, so you can start on a beginner slope and gradually progress to more challenging slopes.It is also important to wear the right equipment when learning how to ski. This includes a helmet, goggles, and warm clothing. Wearing the right\n\n\n\nLoRA微调数据集准备使用ChangMianRen&#x2F;Telecom_Fraud_Texts_5，其中包含了大量经过标记的诈骗短信和正常短信样本。\n将数据进行预处理，得到符合LoRA微调格式的数据集。\n原始数据整理为形如：\n\n\n\ncontent\nlabel\n\n\n\n最后小时，在微信添加朋友中输入良品铺子美食旅行关注参与活动并抢最高DIGIT元红包。如需退订请回复TD或直接退出良品铺子的公众号即可！\n0\n\n\n你好，我是贷款公司的代表。你是否有资金需求？我们提供低利率、快速审批的贷款服务。如果你感兴趣的话请添加我的微信号：xxxxxxxxx。\n1\n\n\n你好，是满梦园吗？我这里是公安机关的民警。我们发现您的身份信息可能被泄露了，涉嫌诈骗活动。我们需要您协助调查此事。请下载我们的”teams”app并与我们在上面进行交流。谢谢配合！\n1\n\n\n应用的模版为：\n&quot;&quot;&quot;            ### Instruction:            你是一个专门识别诈骗短信的专家，请判断输入的短信是否是诈骗短信，如果是，请回答True，否则回答False。            诈骗短信一般具有以下特征：            1. 诱导点击链接或拨打电话或添加微信            2. 内容涉及赌博、中奖、钱财等            3. 使用特殊符号或文字，或使用符号隔断文字            4. 使用黑话/暗语，令人难以理解            ### Input:&#123;&#125;            ### Response:&#123;&#125;            &lt;/s&gt;&quot;&quot;&quot;\n\n将数据中的content作为input，label为1时Response为True，为0时Response为False。\n微调代码训练器的参数意义可以参考huggingface transformers使用指南之二——方便的trainer - 知乎\nfrom peft import get_peft_model, LoraConfig, TaskTypefrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArgumentsfrom trl import SFTTrainer,SFTConfigfrom torch.utils.data import Datasetimport pandas as pdclass SMSDataset(Dataset):    def __init__(self, data_path):        self.data = pd.read_csv(data_path)        self.prompt_template = &quot;&quot;&quot;            ### Instruction:            你是一个专门识别诈骗短信的专家，请判断输入的短信是否是诈骗短信，如果是，请回答True，否则回答False。            诈骗短信一般具有以下特征：            1. 诱导点击链接或拨打电话或添加微信            2. 内容涉及赌博、中奖、钱财等            3. 使用特殊符号或文字，或使用符号隔断文字            4. 使用黑话/暗语，令人难以理解            ### Input:&#123;&#125;            ### Response:&#123;&#125;            &lt;/s&gt;&quot;&quot;&quot;    def __len__(self):        return len(self.data)        def __getitem__(self, idx):        raw_data = self.data.iloc[idx]        prompt_data=self.prompt_template.format(raw_data[&#x27;content&#x27;],&quot;True&quot; if raw_data[&#x27;label&#x27;]==1 else &quot;False&quot;)        prompt_data=tokenizer(prompt_data)        return prompt_dataSMStrainDataset = SMSDataset(&quot;./train.csv&quot;)SMSvalidDataset = SMSDataset(&quot;./valid.csv&quot;)model_path = &quot;/home/data/pre_model/Llama-2-7b-hf&quot;model = AutoModelForCausalLM.from_pretrained(    model_path,    device_map=&quot;auto&quot;,           # load_in_8bit=True)   model.enable_input_require_grads()tokenizer = AutoTokenizer.from_pretrained(model_path)tokenizer.pad_token = tokenizer.eos_tokenlora_config = LoraConfig(    task_type=TaskType.CAUSAL_LM,      inference_mode=False,              r=8,                               lora_alpha=16,                     lora_dropout=0.1,              )model = get_peft_model(model, lora_config)model.print_trainable_parameters()training_args = SFTConfig(    per_device_train_batch_size=1,    gradient_accumulation_steps=4,    warmup_steps = 5,    num_train_epochs = 1,    gradient_checkpointing=True,    #max_steps = 60,    learning_rate = 2e-4,    optim = &quot;adamw_torch&quot;,    weight_decay = 0.01,    lr_scheduler_type = &quot;cosine&quot;,    seed = 3407,    output_dir = &quot;./results&quot;,    report_to = &quot;none&quot;,    max_seq_length = 512,    dataset_num_proc = 4,    packing = False, )trainer = SFTTrainer(    model=model,                  tokenizer=tokenizer,         args=training_args,                 train_dataset=SMStrainDataset,        eval_dataset=SMSvalidDataset,    peft_config=lora_config,)trainer.train()model.save_pretrained(&#x27;./lora_model&#x27;)\n\n值得注意的是，过程中出现了张量不在同一设备的情况，经过检查，在transformers库的loss_utils.py文件内的\nForCausalLMLoss函数内增加\nnum_items_in_batch=num_items_in_batch.to(logits.device)\n\n解决了设备不同的问题。\n效果验证构造测试脚本进行测试，取模型输出的前五个字符作为判断结果\nrsp=output[len(input_text):].strip()if &quot;True&quot; in rsp[:5] and label==True:    current+=1elif &quot;False&quot; in rsp[:5] and label==False:    current+=1\n\n对比原始模型和微调后模型结果如下：\n\n\n\n指标\n原始模型\n微调后模型\n\n\n\n准确率\n0.180\n0.977\n\n\nF1分数\n0.294\n0.968\n\n\n","categories":["LLM实践","LoRA"]},{"title":"TinyLLM学习日记","url":"//posts/2025/05/blog2/","content":"我希望通过训练一个TinyLLM来打好大模型训练的基础，过程中会遇到很多问题，因此在这里记录学习日记。比较重要的是，要学习理解好一些封装好的接口的使用，避免知其然不知其所以然。\n本部分的教程使用的是datawhalechina&#x2F;tiny-universe: 《大模型白盒子构建指南》：一个全手搓的Tiny-Universe中的TinyLLM部分，不会再对原教程赘述，只记录自己的思考以及扩展实验。\nStep 1: 训练TokenizerSentencePiece库的使用预备知识TinyLLM使用了 SentencePiece 库来训练自定义的 Tokenizer，我希望增进对其的理解，参考资料：大模型词表扩充必备工具SentencePiece - 知乎。\n\nTokenizer有三种粒度：word&#x2F;character&#x2F;subword\n\nsubword平衡了两种方法，常见的子词算法有Byte-Pair Encoding (BPE) &#x2F; Byte-level BPE（BBPE）、Unigram LM、WordPiece、SentencePiece等。\n\nBPE，即字节对编码。其核心思想是从字母开始，不断找词频最高、且连续的两个token合并，直到达到目标词数。\nBBPE的核心思想是将BPE从字符级别扩展到子节（Byte）级别。BPE的一个问题是如果遇到了unicode编码，基本字符集可能会很大。BBPE就是以一个字节为一种“字符”，不管实际字符集用了几个字节来表示一个字符。这样的话，基础字符集的大小就锁定在了256（2^8）。采用BBPE的好处是可以跨语言共用词表，显著压缩词表的大小。而坏处就是，对于类似中文这样的语言，一段文字的序列长度会显著增长。因此，BBPE based模型可能比BPE based模型表现的更好。然而，BBPE sequence比起BPE来说略长，这也导致了更长的训练&#x2F;推理时间。BBPE其实与BPE在实现上并无大的不同，只不过基础词表使用256的字节集。\n\n\n\nSentencePiece 特性\n固定最终词汇表大小\n使用原始句子训练\n空格被视为基本符号 “▁” ，因此可以无歧义地对文本进行detokenize\n\nSentencePiece 实验使用一个简单的示例进行测试：“aa bb cc aab abbd bb.”\n测试代码如下：\nimport sentencepiece as spmdataset_path = &#x27;./demo.txt&#x27;vocab_size=10spm.SentencePieceTrainer.train(input=dataset_path,model_type=&quot;bpe&quot;, model_prefix=&#x27;demo&#x27;, vocab_size=vocab_size)sp = spm.SentencePieceProcessor()sp.load(&#x27;demo.model&#x27;)text=&#x27;aa bb cc aab abbd bb.&#x27;print(sp.encode_as_pieces(text))print(sp.encode_as_ids(text))for i in range(vocab_size):    print(i,sp.id_to_piece(i))\n\n使用该代码可以看到分词器对这一简单句子的分词结果。\n设置vocab_size小于9时，会报错\nRuntimeError: Internal: src/trainer_interface.cc(582) [(static_cast&lt;int&gt;(required_chars_.size() + meta_pieces_.size())) &lt;= (trainer_spec_.vocab_size())] Vocabulary size is smaller than required_chars. 8 vs 9. Increase vocab_size or decrease character_coverage with --character_coverage option.\n\n这是因为SentencePieceTrainer会自动添加未知符： 、BOS：&lt;s&gt;、EOS：&lt;&#x2F;s&gt;、▁，加上这个例子本来的5个字符，需要至少9个字符才能分词。\n而设置的上限即分词算法能计算到最大标记总数，例如，考虑一个简单的例子“ab ac bc cd de”，其上限是：字符总数（5）+\n”▁？“型（4）+”▁？？“型（5）+”？？“型（5）+自动添加（4）&#x3D;23。\n一般遇到设定词典大小过大的问题时，可能是数据不够丰富导致的，这时可以选择增加数据或者减少词典大小。\n分词器训练与使用\n训练：（参数文档参考sentencepiece&#x2F;doc&#x2F;options.md at master · google&#x2F;sentencepiece）\n\nspm.SentencePieceTrainer.train(        input=tiny_file,         # 输入文件为之前生成的 tiny.txt        model_prefix=prefix,     # 模型前缀路径        model_type=&quot;bpe&quot;,        # 使用 Byte-Pair Encoding (BPE) 训练分词器        vocab_size=vocab_size,   # 词汇表大小        self_test_sample_size=0, # 自测样本大小设置为 0        input_format=&quot;text&quot;,     # 输入文件格式为纯文本        character_coverage=1.0,  # 覆盖所有字符（包括非常见字符）        num_threads=os.cpu_count(),  # 使用 CPU 的线程数        split_digits=True,       # 拆分数字        allow_whitespace_only_pieces=True,  # 允许仅由空格组成的词元        byte_fallback=True,      # 启用字节级回退        unk_surface=r&quot; \\342\\201\\207 &quot;,  # UNK token 表示未知字符的方式        normalization_rule_name=&quot;identity&quot;  # 使用“identity”归一化规则    )\n\n\n加载：\n\nsp_model = SentencePieceProcessor(model_file=model_path)\n\n\n编码：(s：str)\n\nsp_model.encode(s)\n\n\n解码：(t: List[int])\n\nsp_model.decode(t)\n\n\n\n\n\nStep 2: 数据预处理functools.partialfunctools.partial 是 Python 标准库中 functools 模块提供的一个高阶函数，主要用于部分应用函数参数。它允许固定函数的部分参数，生成一个新的简化版函数，从而减少后续调用时的参数传递量。\n代码将process_shard(args, vocab_size, tokenizer_model_path)\n封装为fun = partial(process_shard, vocab_size=vocab_size, tokenizer_model_path=TOKENIZER_MODEL)\n则后续调用时形如fun((0,&#39;path&#39;))，传入一个元组\n预处理将文本数据使用Step1训练的分词器转换为数字序列，并编码为可训练的格式（为每一段文本添加BOS），最后以二进制形式保存\n加载已预处理好的数据集TinyLLM中设计了一个 PretokDataset 类\n核心加载数据的代码如下：\nwhile True:    # 随机打乱分片文件    rng.shuffle(shard_filenames)    for shard in shard_filenames:        # 使用 memmap 读取文件，使得数据留在磁盘上，减少内存占用        m = np.memmap(shard, dtype=np.uint16, mode=&quot;r&quot;)        # 计算该分片中的批次数量        num_batches = len(m) // self.max_seq_len        num_batches -= 1  # 去掉最后一个不完整的批次        assert num_batches &gt; 0, &quot;这个分片文件太小了？请检查。&quot;        # 随机打乱批次索引        ixs = list(range(num_batches))        rng.shuffle(ixs)        # 对每个批次生成输入 x 和目标输出 y        for ix in ixs:            start = ix * self.max_seq_len  # 批次起始索引            end = start + self.max_seq_len + 1  # 批次结束索引            # 将数据转换为 NumPy 数组并拷贝到 RAM 中            chunk = torch.from_numpy((m[start:end]).astype(np.int64))            # 模型输入 x 是当前批次的前 max_seq_len 个词元            x = chunk[:-1]            # 模型输出 y 是下一个词元            y = chunk[1:]            # 生成 x, y 对            yield x, y\n\n可以看出这里使用的是步长与窗口大小相等的滑动窗口采样方法，之后可以尝试修改这部分数据加载机制以更大程度地利用数据。\nStep 3: 训练模型TInyLLM使用的模型是与 LLaMA2 结构相同的 Decoder-only Transformer 模型，此部分根据源码进行解读分析。\n在最基本的大模型架构基础上，使用了以下策略：\n\n对残差投影进行特殊的缩放初始化\n\nfor pn, p in self.named_parameters():    if pn.endswith(&#x27;w3.weight&#x27;) or pn.endswith(&#x27;wo.weight&#x27;):        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * args.n_layers))\n\n这是对DecoderLayer内的MLP层的第三层线性变换和Attention层的输出权重矩阵进行放缩\n\n旋转编码\n\n参考十分钟读懂旋转编码（RoPE）\nTinyLLM这里的实现与LLAMA里的一致，之后再专门研究学习一下编码。\n\n学习率调整\n\n包括线性预热、余弦退火和最小学习率限制。\n\n自动混合精度训练\n\n参考【Trick2】torch.cuda.amp自动混合精度训练 —— 节省显存并加快推理速度_torch.cuda.amp.gradscaler()-CSDN博客。\n因为在某些上下文中torch.FloatTensor有优势，有的torch.HalfTensor有优势。动态估计的原理就是在不出现inf或者NaN梯度值的情况下尽可能增大scaler的值。在每次scaler.step(optimizer)中，都会检查是否有inf或NaN的梯度出现：\n\n如果出现了inf或者NaN，scaler.step(optimizer)会忽略此次的权重更新（optimizer.step() )，并且将scaler的大小缩小（乘上backoff_factor）；\n如果没有出现inf或者NaN，那么权重正常更新，并且当连续多次（growth_interval指定）没有出现inf或者NaN，则scaler.update()会将scaler的大小增加（乘上growth_factor）。\n\n# 实例化一个GradScaler对象scaler = amp.GradScaler(enabled=True)# 将梯度放大 防止梯度消失scaler.scale(loss).backward()# 更新优化器和梯度缩放器scaler.step(optimizer)scaler.update()\n\n\n\nStep 4: 使用模型生成文本","categories":["LLM实践","TinyLLM"]}]