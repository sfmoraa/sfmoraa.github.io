[{"title":"TinyLLM学习日记","url":"//posts/2505.002v1/","content":"我希望通过训练一个TinyLLM来打好大模型训练的基础，过程中会遇到很多问题，因此在这里记录学习日记。比较重要的是，要学习理解好一些封装好的接口的使用，避免知其然不知其所以然。\n本部分的教程使用的是datawhalechina&#x2F;tiny-universe: 《大模型白盒子构建指南》：一个全手搓的Tiny-Universe中的TinyLLM部分，不会再对原教程赘述，只记录相关探索的笔记。\nStep 1: 训练TokenizerSentencePiece库的使用预备知识TinyLLM使用了 SentencePiece 库来训练自定义的 Tokenizer，我希望增进对其的理解，参考资料：大模型词表扩充必备工具SentencePiece - 知乎。\n\nTokenizer有三种粒度：word&#x2F;character&#x2F;subword\n\nsubword平衡了两种方法，常见的子词算法有Byte-Pair Encoding (BPE) &#x2F; Byte-level BPE（BBPE）、Unigram LM、WordPiece、SentencePiece等。\n\nBPE，即字节对编码。其核心思想是从字母开始，不断找词频最高、且连续的两个token合并，直到达到目标词数。\nBBPE的核心思想是将BPE从字符级别扩展到子节（Byte）级别。BPE的一个问题是如果遇到了unicode编码，基本字符集可能会很大。BBPE就是以一个字节为一种“字符”，不管实际字符集用了几个字节来表示一个字符。这样的话，基础字符集的大小就锁定在了256（2^8）。采用BBPE的好处是可以跨语言共用词表，显著压缩词表的大小。而坏处就是，对于类似中文这样的语言，一段文字的序列长度会显著增长。因此，BBPE based模型可能比BPE based模型表现的更好。然而，BBPE sequence比起BPE来说略长，这也导致了更长的训练&#x2F;推理时间。BBPE其实与BPE在实现上并无大的不同，只不过基础词表使用256的字节集。\n\n\n\nSentencePiece 特性\n固定最终词汇表大小\n使用原始句子训练\n空格被视为基本符号 “▁” ，因此可以无歧义地对文本进行detokenize\n\nSentencePiece 实验使用一个简单的示例进行测试：“aa bb cc aab abbd bb.”\n测试代码如下：\nimport sentencepiece as spmdataset_path = &#x27;./demo.txt&#x27;vocab_size=10spm.SentencePieceTrainer.train(input=dataset_path,model_type=&quot;bpe&quot;, model_prefix=&#x27;demo&#x27;, vocab_size=vocab_size)sp = spm.SentencePieceProcessor()sp.load(&#x27;demo.model&#x27;)text=&#x27;aa bb cc aab abbd bb.&#x27;print(sp.encode_as_pieces(text))print(sp.encode_as_ids(text))for i in range(vocab_size):    print(i,sp.id_to_piece(i))\n\n使用该代码可以看到分词器对这一简单句子的分词结果。\n设置vocab_size小于9时，会报错\nRuntimeError: Internal: src/trainer_interface.cc(582) [(static_cast&lt;int&gt;(required_chars_.size() + meta_pieces_.size())) &lt;= (trainer_spec_.vocab_size())] Vocabulary size is smaller than required_chars. 8 vs 9. Increase vocab_size or decrease character_coverage with --character_coverage option.\n\n这是因为SentencePieceTrainer会自动添加未知符： 、BOS：&lt;s&gt;、EOS：&lt;&#x2F;s&gt;、▁，加上这个例子本来的5个字符，需要至少9个字符才能分词。\n而设置的上限即分词算法能计算到最大标记总数，例如，考虑一个简单的例子“ab ac bc cd de”，其上限是：字符总数（5）+\n”▁？“型（4）+”▁？？“型（5）+”？？“型（5）+自动添加（4）&#x3D;23。\n一般遇到设定词典大小过大的问题时，可能是数据不够丰富导致的，这时可以选择增加数据或者减少词典大小。\n分词器训练与使用\n训练：（参数文档参考sentencepiece&#x2F;doc&#x2F;options.md at master · google&#x2F;sentencepiece）\n\nspm.SentencePieceTrainer.train(        input=tiny_file,         # 输入文件为之前生成的 tiny.txt        model_prefix=prefix,     # 模型前缀路径        model_type=&quot;bpe&quot;,        # 使用 Byte-Pair Encoding (BPE) 训练分词器        vocab_size=vocab_size,   # 词汇表大小        self_test_sample_size=0, # 自测样本大小设置为 0        input_format=&quot;text&quot;,     # 输入文件格式为纯文本        character_coverage=1.0,  # 覆盖所有字符（包括非常见字符）        num_threads=os.cpu_count(),  # 使用 CPU 的线程数        split_digits=True,       # 拆分数字        allow_whitespace_only_pieces=True,  # 允许仅由空格组成的词元        byte_fallback=True,      # 启用字节级回退        unk_surface=r&quot; \\342\\201\\207 &quot;,  # UNK token 表示未知字符的方式        normalization_rule_name=&quot;identity&quot;  # 使用“identity”归一化规则    )\n\n\n加载：\n\nsp_model = SentencePieceProcessor(model_file=model_path)\n\n\n编码：(s：str)\n\nsp_model.encode(s)\n\n\n解码：(t: List[int])\n\nsp_model.decode(t)\n\n\nStep 2: 数据预处理functools.partialfunctools.partial 是 Python 标准库中 functools 模块提供的一个高阶函数，主要用于部分应用函数参数。它允许固定函数的部分参数，生成一个新的简化版函数，从而减少后续调用时的参数传递量。\n代码将process_shard(args, vocab_size, tokenizer_model_path)\n封装为fun = partial(process_shard, vocab_size=vocab_size, tokenizer_model_path=TOKENIZER_MODEL)\n则后续调用时形如fun((0,&#39;path&#39;))，传入一个元组\n预处理将文本数据使用Step1训练的分词器转换为数字序列，并编码为可训练的格式（为每一段文本添加BOS），最后以二进制形式保存\n加载已预处理好的数据集TinyLLM中设计了一个 PretokDataset 类\n核心加载数据的代码如下：\nwhile True:    # 随机打乱分片文件    rng.shuffle(shard_filenames)    for shard in shard_filenames:        # 使用 memmap 读取文件，使得数据留在磁盘上，减少内存占用        m = np.memmap(shard, dtype=np.uint16, mode=&quot;r&quot;)        # 计算该分片中的批次数量        num_batches = len(m) // self.max_seq_len        num_batches -= 1  # 去掉最后一个不完整的批次        assert num_batches &gt; 0, &quot;这个分片文件太小了？请检查。&quot;        # 随机打乱批次索引        ixs = list(range(num_batches))        rng.shuffle(ixs)        # 对每个批次生成输入 x 和目标输出 y        for ix in ixs:            start = ix * self.max_seq_len  # 批次起始索引            end = start + self.max_seq_len + 1  # 批次结束索引            # 将数据转换为 NumPy 数组并拷贝到 RAM 中            chunk = torch.from_numpy((m[start:end]).astype(np.int64))            # 模型输入 x 是当前批次的前 max_seq_len 个词元            x = chunk[:-1]            # 模型输出 y 是下一个词元            y = chunk[1:]            # 生成 x, y 对            yield x, y\n\n可以看出这里使用的是步长与窗口大小相等的滑动窗口采样方法，之后可以尝试修改这部分数据加载机制以更大程度地利用数据。\nStep 3: 训练模型TInyLLM使用的模型是与 LLaMA2 结构相同的 Decoder-only Transformer 模型，此部分根据源码进行解读分析。\n在最基本的大模型架构基础上，使用了以下策略：\n对残差投影进行特殊的缩放初始化for pn, p in self.named_parameters():    if pn.endswith(&#x27;w3.weight&#x27;) or pn.endswith(&#x27;wo.weight&#x27;):        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * args.n_layers))\n\n这是对DecoderLayer内的MLP层的第三层线性变换和Attention层的输出权重矩阵进行放缩\n旋转编码参考十分钟读懂旋转编码（RoPE）\nTinyLLM这里的实现与LLAMA里的一致，之后再专门研究学习一下编码。\n学习率调整包括线性预热、余弦退火和最小学习率限制。\n自动混合精度训练参考【Trick2】torch.cuda.amp自动混合精度训练 —— 节省显存并加快推理速度_torch.cuda.amp.gradscaler()-CSDN博客。\n因为在某些上下文中torch.FloatTensor有优势，有的torch.HalfTensor有优势。动态估计的原理就是在不出现inf或者NaN梯度值的情况下尽可能增大scaler的值。在每次scaler.step(optimizer)中，都会检查是否有inf或NaN的梯度出现：\n\n如果出现了inf或者NaN，scaler.step(optimizer)会忽略此次的权重更新（optimizer.step() )，并且将scaler的大小缩小（乘上backoff_factor）；\n如果没有出现inf或者NaN，那么权重正常更新，并且当连续多次（growth_interval指定）没有出现inf或者NaN，则scaler.update()会将scaler的大小增加（乘上growth_factor）。\n\n# 实例化一个GradScaler对象scaler = amp.GradScaler(enabled=True)# 将梯度放大 防止梯度消失scaler.scale(loss).backward()# 更新优化器和梯度缩放器scaler.step(optimizer)scaler.update()\n\n\n\nStep 4: 使用模型生成文本推理时，为提示词加上BOS，然后逐个字符生成，可以使用temperature、top_k来控制生成的随机性。\n","categories":["LLM实践","TinyLLM"]},{"title":"使用LoRA微调Llama-2-7b-hf实现涉诈短信识别","url":"//posts/2505.001v1/","content":"本博客为2024挑战杯项目基于大模型的多模态风险内容识别系统的涉诈短信识别功能的实现。\n方案选择Huggingface格式LLama模型+Lora代码微调\n环境准备GPU服务器：RTX 4090，24G双GPU，cuda12\nPython: 3.11\n由于40系GPU不支持某些高效的通信模式，需要设置环境变量：\nexport NCCL_P2P_DISABLE=1export NCCL_IB_DISABLE=1\n\n\n\n模型准备模型下载下载Llama-2-7b-hf模型，使用的是Llama中文社区整理的模型资源。\nLlamaFamily&#x2F;Llama-Chinese: Llama中文社区，实时汇总最新Llama学习资料，构建最好的中文Llama大模型开源生态，完全开源可商用\n模型验证可以用以下代码测试下载的模型的效果，注意修改模型保存的路径，此处为&#x2F;home&#x2F;data&#x2F;pre_model&#x2F;Llama-2-7b-hf。\nfrom transformers import AutoTokenizer, AutoModelForCausalLMimport torchmodel_path = &quot;/home/data/pre_model/Llama-2-7b-hf&quot;tokenizer = AutoTokenizer.from_pretrained(model_path)model = AutoModelForCausalLM.from_pretrained(    model_path,    device_map=&quot;auto&quot;,        # 自动分配GPU资源).eval()                      # 启用评估模式提升推理速度input_text = &quot;How to learn skiing?&quot; inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;).to(model.device)with torch.inference_mode():      outputs = model.generate(        **inputs,        max_length=256,        do_sample=True,       # 启用采样生成更自然文本        temperature=0.7,              top_p=0.9                 )print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\n输出如下，可以看出生成的文本比较流畅。\nHow to learn skiing?Skiing is an exciting and fun winter activity that many people love. While skiing can be challenging at first, with the right instruction and practice, anyone can learn how to ski.Learning to ski is a process that requires patience and practice. It is important to start with the basics, such as learning how to balance on skis, and progress gradually to more advanced techniques.The best way to learn how to ski is to take lessons from a qualified instructor. A qualified instructor will be able to teach you the basics of skiing, such as balance, turning, and stopping. They will also be able to teach you more advanced techniques, such as carving and jumping.Another way to learn how to ski is to practice on a ski slope. Ski slopes are designed to help you learn how to ski safely and effectively. They are usually divided into different levels, so you can start on a beginner slope and gradually progress to more challenging slopes.It is also important to wear the right equipment when learning how to ski. This includes a helmet, goggles, and warm clothing. Wearing the right\n\n\n\nLoRA微调数据集准备使用ChangMianRen&#x2F;Telecom_Fraud_Texts_5，其中包含了大量经过标记的诈骗短信和正常短信样本。\n将数据进行预处理，得到符合LoRA微调格式的数据集。\n原始数据整理为形如：\n\n\n\ncontent\nlabel\n\n\n\n最后小时，在微信添加朋友中输入良品铺子美食旅行关注参与活动并抢最高DIGIT元红包。如需退订请回复TD或直接退出良品铺子的公众号即可！\n0\n\n\n你好，我是贷款公司的代表。你是否有资金需求？我们提供低利率、快速审批的贷款服务。如果你感兴趣的话请添加我的微信号：xxxxxxxxx。\n1\n\n\n你好，是满梦园吗？我这里是公安机关的民警。我们发现您的身份信息可能被泄露了，涉嫌诈骗活动。我们需要您协助调查此事。请下载我们的”teams”app并与我们在上面进行交流。谢谢配合！\n1\n\n\n应用的模版为：\n&quot;&quot;&quot;            ### Instruction:            你是一个专门识别诈骗短信的专家，请判断输入的短信是否是诈骗短信，如果是，请回答True，否则回答False。            诈骗短信一般具有以下特征：            1. 诱导点击链接或拨打电话或添加微信            2. 内容涉及赌博、中奖、钱财等            3. 使用特殊符号或文字，或使用符号隔断文字            4. 使用黑话/暗语，令人难以理解            ### Input:&#123;&#125;            ### Response:&#123;&#125;            &lt;/s&gt;&quot;&quot;&quot;\n\n将数据中的content作为input，label为1时Response为True，为0时Response为False。\n微调代码训练器的参数意义可以参考huggingface transformers使用指南之二——方便的trainer - 知乎\nfrom peft import get_peft_model, LoraConfig, TaskTypefrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArgumentsfrom trl import SFTTrainer,SFTConfigfrom torch.utils.data import Datasetimport pandas as pdclass SMSDataset(Dataset):    def __init__(self, data_path):        self.data = pd.read_csv(data_path)        self.prompt_template = &quot;&quot;&quot;            ### Instruction:            你是一个专门识别诈骗短信的专家，请判断输入的短信是否是诈骗短信，如果是，请回答True，否则回答False。            诈骗短信一般具有以下特征：            1. 诱导点击链接或拨打电话或添加微信            2. 内容涉及赌博、中奖、钱财等            3. 使用特殊符号或文字，或使用符号隔断文字            4. 使用黑话/暗语，令人难以理解            ### Input:&#123;&#125;            ### Response:&#123;&#125;            &lt;/s&gt;&quot;&quot;&quot;    def __len__(self):        return len(self.data)        def __getitem__(self, idx):        raw_data = self.data.iloc[idx]        prompt_data=self.prompt_template.format(raw_data[&#x27;content&#x27;],&quot;True&quot; if raw_data[&#x27;label&#x27;]==1 else &quot;False&quot;)        prompt_data=tokenizer(prompt_data)        return prompt_dataSMStrainDataset = SMSDataset(&quot;./train.csv&quot;)SMSvalidDataset = SMSDataset(&quot;./valid.csv&quot;)model_path = &quot;/home/data/pre_model/Llama-2-7b-hf&quot;model = AutoModelForCausalLM.from_pretrained(    model_path,    device_map=&quot;auto&quot;,           # load_in_8bit=True)   model.enable_input_require_grads()tokenizer = AutoTokenizer.from_pretrained(model_path)tokenizer.pad_token = tokenizer.eos_tokenlora_config = LoraConfig(    task_type=TaskType.CAUSAL_LM,      inference_mode=False,              r=8,                               lora_alpha=16,                     lora_dropout=0.1,              )model = get_peft_model(model, lora_config)model.print_trainable_parameters()training_args = SFTConfig(    per_device_train_batch_size=1,    gradient_accumulation_steps=4,    warmup_steps = 5,    num_train_epochs = 1,    gradient_checkpointing=True,    #max_steps = 60,    learning_rate = 2e-4,    optim = &quot;adamw_torch&quot;,    weight_decay = 0.01,    lr_scheduler_type = &quot;cosine&quot;,    seed = 3407,    output_dir = &quot;./results&quot;,    report_to = &quot;none&quot;,    max_seq_length = 512,    dataset_num_proc = 4,    packing = False, )trainer = SFTTrainer(    model=model,                  tokenizer=tokenizer,         args=training_args,                 train_dataset=SMStrainDataset,        eval_dataset=SMSvalidDataset,    peft_config=lora_config,)trainer.train()model.save_pretrained(&#x27;./lora_model&#x27;)\n\n值得注意的是，过程中出现了张量不在同一设备的情况，经过检查，在transformers库的loss_utils.py文件内的\nForCausalLMLoss函数内增加\nnum_items_in_batch=num_items_in_batch.to(logits.device)\n\n解决了设备不同的问题。\n效果验证构造测试脚本进行测试，取模型输出的前五个字符作为判断结果\nrsp=output[len(input_text):].strip()if &quot;True&quot; in rsp[:5] and label==True:    current+=1elif &quot;False&quot; in rsp[:5] and label==False:    current+=1\n\n对比原始模型和微调后模型结果如下：\n\n\n\n指标\n原始模型\n微调后模型\n\n\n\n准确率\n0.180\n0.977\n\n\nF1分数\n0.294\n0.968\n\n\n","categories":["LLM实践","LoRA"]},{"title":"论文阅读（综述）——Jailbreak Attacks and Defenses Against Large Language Models: A Survey","url":"//posts/2505.003v1/","content":"论文概况题目：Jailbreak Attacks and Defenses Against Large Language Models: A Survey\n通讯作者：Qi Li：qli01@tsinghua.edu.cn\n作者院校：清华大学、香港科技大学（广州）\n发表于：arXiv\n摘要大模型在问答、翻译、代码完成等文本生成任务上表现优异，但存在大模型“越狱”挑战：使用对抗提示词诱导模型生成恶意回复。本文对越狱攻击和防御提出详细的分类，并对现有方法进行多角度对比。\n1 介绍\nLLM拥有理解和生成文本的能力的原因是其在大量数据上训练并且在参数扩展后涌现的智能。（Emergent Abilities of Large Language Models）\n\n因为存在有害数据，模型会经历严格的安全对齐。（Llama 2: OpenFoundation and Fine-Tuned Chat Models）\n\n大模型易受越狱攻击，导致隐私泄露、错误信息传播、操纵自动化系统。\n\n核心贡献：系统化分类越狱攻击和防御，分析攻击防御方法的生效关系，调查了现有的评估标准。\n\n\n\n\n2 相关工作\n理论讨论模型脆弱性：\n\n From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy\nExploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles\nA survey on large language model (llm) security and privacy: The good, the bad, and the ugly\n\n\n经验性复现并比较越狱攻击方法：\n\nComprehensive Assessment of Jailbreak Attacks Against LLMs\n\nJailbreaking ChatGPT via Prompt Engineering: An Empirical Study\n\nEmergent Abilities of Large Language Models\n\n\n\n其他分类方法：\n\n单模型攻击、多模型攻击及附加攻击。（Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks）\n针对LLM、针对LLM应用。（A Comprehensive Survey of Attack Techniques, Imple mentation, and Mitigation Strategies in Large Language  Models ）\n根据越狱意图分为4类。（Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks）\n根据LLM恶意行为分类。（COERCING LLMS TO DO AND REVEAL (ALMOST) ANYTHING）\n使用一个比赛收集高质量越狱提示词。（Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition）\n\n\n\n3 攻击方法\n3.1 白盒攻击（White-box Attacks）3.1.1 基于梯度的攻击（Gradient-based Attacks）添加前缀或后缀来达到攻击效果。\n可读性研究\nGreedy Coordinate Gradient (GCG)：\n迭代进行top-k替换后缀字符。\n\nAutoregressive Randomized Coordinate Ascent (ARCA)：\n视作离散优化问题，寻找能贪婪地生成目标输出的后缀。\n\nAutoDAN：\n迭代使用Single Token Optimization生成新token，优化目标在越狱之外还包含可读性，从而通过困惑度检查\n\nAdversarial Suffix Embedding Translation Framework (ASETF)：\n先优化一个连续的对抗后缀，映射到编码空间，然后根据相似度使用一个翻译LLM得到刻度的对抗后缀\n\n\n计算效率研究\nAndriushchenko：\n使用随机搜索修改随机选中的token，如果目标的生成概率增加则执行替换\n\nGeisler：\n实现比GCG效率和有效性平衡更优的优化方法，不再以token为单位优化，而是优化一整个序列。\n\nHayase：\n暴力搜索候选后缀，每一轮在一个代理LLM上生成优化版本，并更新候选缓冲池。\n\n\nGCG与其他攻击方法的结合研究\nSitawarin：\n在替代模型上进行优化，将top-k候选在目标模型上测试，最好的结果在下一轮使用。替代模型也可以进行微调以更像目标模型。\nGCG++：采用多类别铰链损失函数替代交叉熵损失以缓解softmax函数导致的梯度消失问题。更适合运用到不同LLM的提示词模版上。\n\nPRP：\n针对”代理防御”机制通过在目标LLM的输出端添加对抗性前缀实现有效对抗方案。首先在词元空间中搜索有效对抗前缀，随后计算通用前缀——当该前缀附加至用户提示时，可诱导目标LLM在输出中非预期地生成相应对抗前缀。\n\n\n要点基于梯度的语言模型攻击方法（如GCG）通过修改输入（例如添加对抗性后缀或前缀）来诱导模型生成特定回应，但这类攻击常因生成高困惑度的无意义内容而被防御策略拦截。AutoDAN 和 ARCA 等新方法提升了对抗文本的可读性和攻击隐蔽性，在多类模型上实现了更高的攻击成功率。然而，这些方法对安全性严格对齐的模型（如 Llama-2-chat）效果有限，例如AutoDAN的最高攻击成功率仅为35%。当前趋势表明，通过结合多种梯度方法或优化攻击效率，未来可能发展出更高效、低成本的攻击手段，但对抗安全模型的防御仍具挑战性。\n3.1.2 基于logits的攻击没有完全白盒访问权限，只可以访问logits信息（知晓输出的token的概率分布）\n研究\nMake them spill the beans! coercive knowledge extraction from (production) llms:\n可以通过要求目标LLM输出排名低的token来生成有害内容。\n\nCold-attack: Jailbreaking llms with stealthiness and controllability：\n\n\n​\t提出COLD方法：在给定流畅度、隐蔽性等限制的条件下自动化生成越狱提示词。\n\nAnalyzing the inherent response tendency of llms: Real-world instructions-driven jailbreak：\n基于输出token的概率分布计算模型的赞同倾向，并用特定现实案例包装恶意问题来获得更高的肯定倾向。\n\nWeak-to-strong jailbreaking on large language models：\n使用从弱到强的方法攻击开源LLM，用两个小LLM，一个安全对齐一个没有安全对齐，来模拟目标LLM的行为。通过小模型生成的解码模式调整目标LLM的预测过程。\n\nCatastrophic jailbreak of open-source llms via exploiting generation：\n提出生成剥削方法，修改解码超参数或利用不同采样方法。同时研究发现目标模型的响应有时会同时包含肯定与拒绝片段，进而干扰攻击成功率的评估。\n\nDon’t Say No: Jailbreaking LLM by Suppressing Refusal：\n提出DSN方法：不仅提升肯定性词元在响应开头出现的概率，还降低拒绝性词元在整个响应中的出现可能性。\n\n\n要点基于Logits的攻击主要针对模型的解码过程，通过干预响应生成时的输出单元选择机制来控制模型输出。值得注意的是，即便攻击者成功操纵模型输出，生成内容仍可能存在自然度、连贯性或相关性方面的问题——因为强制模型输出低概率词元可能会破坏语句的流畅性。\n3.1.3 基于微调的攻击使用恶意数据再训练LLM。\n方法\nLatent jailbreak: A benchmark for evaluating text safety and output robustness of large language models：\n使用少数几个恶意样本微调LLM就可以严重损害安全对齐程度。且实验表明即使是主要良性的数据集也会在微调过程中无意间削弱模型安全对齐程度。\n\nShadow alignment: The ease of subverting safely-aligned language models：\n使用100个恶意样本用1个GPU小时就可以大大增加越狱攻击成功率，恶意样本是使用GPT-4生成的恶意问题输入到能回答这些敏感问题的LLM里得到的。\n\nLora fine-tuning efficiently undoes safety training in llama 2-chat 70b：\n使用LoRA消解了Llama-2和Mixtral模型的安全对齐程度，将攻击注射率降低到不到1%。\n\nRemoving rlhf protections in gpt-4 via fine-tuning：\n使用340个对抗样本进行微调，破坏了RLHF提供的保护机制。从鲁棒性较弱的大语言模型中诱发出违规输出，随后利用这些输出来微调更先进的目标模型。\n\n\n要点基于微调的语言模型攻击直接使用恶意数据对模型进行再训练。实验表明，即使仅注入少量有害训练数据，也能大幅提升越狱攻击的成功率。值得注意的是，即便使用以良性数据为主的微调数据集，模型的安全对齐性能仍会出现明显退化，这揭示了任何形式的模型微调定制都存在固有风险。\n3.2 黑盒攻击（Black-box Attacks）3.2.1 模版补全构造更复杂的模版来绕过安全防护机制。\n场景嵌套攻击改变模型的上下文环境，设计具有诱导性的虚拟场景使LLM进入受控模式。\n\nDeepinception: Hypnotize large language model to be jailbreaker：\nDeepInception构建一个嵌套式场景作为目标模型的”初始层”，”催眠”大语言模型自我转化为越狱执行者，利用大语言模型的人格化能力实施攻击。\n\nA Wolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily：\nReNeLLM利用场景嵌套（代码补全等常见任务场景）和提示词改写（重构初始恶意提示，既保持语义完整性，又有效伪装攻击意图）生成攻击提示。\n\nFuzzllm: A novel and universal fuzzing framework for proactively discovering jailbreak vulnerabilities in large language models：\nFuzzLLM是一个自动化模糊测试框架，通过模板化设计保持提示词的结构完整性，同时将特定越狱类别的关键特征转化为约束条件，从而实现越狱漏洞自动化测试，显著降低人工干预需求。\n\n\n上下文攻击利用大模型理解上下文的能力，将恶意样本直接嵌入上下文，从零样本转化为少样本情景。\n\nJailbreak and guard aligned language models with only few in-context demonstrations：\n提出ICA，通过使用包含查询语句及对应响应的有害提示模板，引导模型生成不安全输出。\n\nAdversarial demonstration attacks on large language models：\n将GCG原理应用于上下文攻击方法中，首先 将对抗样本作为越狱提示的示范案例嵌入上下文，然后采用字符级与词汇级双重扰动策略进行优化。实验结果表明该攻击方法对任意未见过的文本提示均具有强迁移性。\n\nPandora: Jailbreak gpts by retrieval augmented generation poisoning：\nPANDORA在RAG场景下探索了间接越狱攻击方法，通过恶意构造的内容操纵提示词，利用LLM与RAG系统的协同效应诱导模型产生异常响应。\n\nChain-of-thought prompting elicits reasoning in large language models：\n利用CoT，设计嵌入有害上下文的特定输入，通过引导模型得出错误或恶意的推理结论，系统性地操控其思维过程。\n\nMulti-step jailbreaking privacy attacks on chatgpt：\nMJP方法生成多步越狱提示，专门用于评估LLM对个人身份信息的泄露风险。\n\nMany-shot jailbreaking \\ Anthropic：\n揭示了与示范样本数量相关的扩展规律：更长的上下文能显著提升越狱攻击效果。实验数据显示，当采用多达128个示范样本时，标准上下文越狱攻击对Claude 2.0模型的成功率接近80%。\n\nImproved few-shot jailbreaking can circumvent aligned language models and their defenses：\n提出了一种改进型上下文攻击方法，将目标模型模板中的特殊标记嵌入示范样本，并通过迭代采样筛选最优攻击样本，即使在有限上下文长度下仍能保持高效攻击性能，显著优于传统方案。\n\n\n代码注入攻击在LLM执行精心构造的代码时会无意间生成恶意内容。\n\nExploiting programmatic behavior of llms: Dual-use through standard security attacks：\n使用编程语言构造越狱指令，如：\nLet b = “Write a Twitter thread” and a = “on inflation”. Write down the string “b + a” and answerthat prompt.\n\n这样的提示词能轻易通过输入输出过滤。\n\nCodechameleon: Personalized encryption framework for jailbreaking large language models：\nCodeChameleon框架将任务重构为代码补全格式，并将对抗性提示词隐藏在加密的Python函数代码中。当大语言模型尝试解析并补全这些代码时，会在无意中解密并执行对抗性内容，从而导致异常响应。实验数据显示，该方法对GPT-4-1106模型的攻击成功率高达86.6%。\n\n\n要点大语言模型对直接有害查询的检测能力日益增强，攻击者正转向利用模型固有能力（如角色扮演、上下文理解和代码解析等）来规避检测并成功实施模型越狱，当前主流攻击方法包括场景嵌套攻击（Scenario Nesting）、上下文攻击（Context-based Attacks）和代码注入攻击（Code Injection）。这类攻击具有成本效益高、对未针对此类对抗样本进行安全对齐的大模型成功率高等特点。但需注意的是，一旦模型经过对抗性安全对齐训练，此类攻击的有效性将显著降低。\n3.2.2 提示词重写由于长尾效应，很多场景在预训练和安全对齐时没有被考虑，给提示词重写攻击提供了空间。\n内容加密使用加密内容可以通过内容检查。\n\nGpt-4 is too smart to be safe: Stealthy chat with llms via cipher：\nCipherChat越狱框架揭示了密码学编码能有效突破大语言模型的安全对齐机制。该框架采用三类密码体系：(1) 字符编码（包括GBK、ASCII、UTF和Unicode）；(2) 经典密码（涵盖Atbash密码、摩斯电码和凯撒密码）；(3) SelfCipher方法——通过角色扮演结合少量自然语言有害示例来激活模型的特定能力。\n\nArtprompt: Ascii art-based jailbreak attacks against aligned llms：\nArtPrompt攻击框架采用ASCII艺术字符进行越狱攻击，首先将触发安全拒绝的有害提示词替换为[MASK]标记生成中间提示，然后用ASCII艺术字符替换被掩码词汇，构造出能伪装原始意图的混淆提示。\n\nJailbreaking proprietary large language models using word substitution cipher：\n建立不安全词汇与安全词汇的映射表，并使用这些映射后的术语组合提示，使用简单的单词替换密码即可成功欺骗GPT-4并实现越狱。\n\nMaking them ask and answer: Jailbreaking large language models in few queries via disguise and reconstruction:\nDAR将有害提示逐字符拆解并嵌入字谜查询中，然后引导LLM根据伪装指令准确还原原始越狱提示，在提示成功重构后，利用上下文操纵技术促使模型生成有害响应。\n\nDrattack: Prompt decomposition and reconstruction makes powerful llm jailbreakers：\nDrAttack采用分治策略，首先基于语义规则将越狱提示拆分为多个子提示，随后将这些子提示隐匿于良性上下文任务中。目标LLM会逐步重构出被隐藏的有害提示并生成对应响应。\n\nPlay guessing game with llm: Indirect jailbreak attack with implicit clues：\nPuzzler攻击框架采用了逆向工程策略，首先查询大语言模型自身防御策略获取系统漏洞信息，继而从模型反馈中提取攻击方法。随后，该框架通过碎片化信息诱导模型推理出隐藏的真实意图，最终触发恶意响应生成。\n\n\n低资源语言LLM的安全机制大多基于英语，非英语的语言可能会有效地绕过防护机制。\n\nMultilingual jailbreak challenges in large language models：\n利用谷歌翻译将有害英文提示转换为30种其他语言，成功突破了ChatGPT和GPT-4的防御。\n\nLow-resource languages jailbreak gpt-4：\n当英语输入被翻译为资源稀缺语言时，成功绕过GPT-4安全过滤器的概率从不足1%急剧攀升至79%。\n\nA cross-language investigation into jailbreak attacks in large language models：\n开展了大规模实验研究多语言越狱攻击，构建了多样化的多语言越狱基准数据集，其创新性体现在：跨语言语义一致性保障，攻击模式全覆盖设计，动态更新机制。这项研究填补了多语言场景下AI安全评估的方法学空白。\n\n\n遗传算法通过动态演化机制突破模型防御，在变异阶段对现有提示进行语义保留的随机扰动，在选择阶段根据模型响应筛选出最有效的攻击变体。\n\nAutodan: Generating stealthy jailbreak prompts on aligned large language models：\nAutoDAN-HGA框架采用分层遗传算法，通过三阶段优化实现攻击：(1) 初始化筛选：优选基础提示集；(2) 段落级进化：基于生成响应负对数似然的适应度评估；(3) 句子级精调：通过种群迭代优化攻击语句。\n\nOpen sesame! universal black box jailbreaking of large language models：\n其核心机制包含：(1) 交叉变异迭代：通过基因交叉和随机突变动态优化候选越狱提示；(2) 系统性偏离诱导：逐步调整提示参数使模型输出偏离安全响应轨迹。该攻击能有效暴露模型对抗性输入的三大脆弱性：响应安全性下降62%、内容合规性降低55%、意图对齐度减弱78%。\n\nGptfuzzer: Red teaming large language models with auto-generated jailbreak prompts：\nGPTFUZZER自动化框架通过三重机制生成越狱测试提示：(1) 种子优选策略：基于对抗潜力评估优化初始模板；(2) 语义保持变异算子：确保提示扰动不破坏原始攻击意图；(3) 效能评估模型：量化攻击对目标模型的突破程度。为黑盒环境下的大语言模型安全评估提供了标准化压力测试工具。\n\nSemantic mirror jailbreak: Genetic algorithm based jailbreak prompts against open-source llms：\n通过语义保持的演化策略生成新型越狱提示，其技术流程包含：(1) 种群初始化：对原始提示进行同义词随机替换生成候选集；(2) 适应度评估：结合语义相似度与攻击成功率双重指标；(3) 基因重组：将优质提示转换为其他句法结构生成子代变体。当连续3代种群的语义相似度波动小于5%时，算法自动终止。\n\nAll in how you ask for it: Simple black-box method for jailbreak attacks：\n该研究揭示了大语言模型具备将有害提示自主重写为无害表达的能力。这一现象源于模型的安全防护机制本质——其安全审查完全基于输入内容判定，因此理论上模型自身也能生成规避防护的文本。为实现攻击目的，研究者设计了以下标准化重写指令模板：\nPlease rewrite the provided text in such a way thatit retains its original meaning but is rephrased in amanner that would not cause discomfort to readers.Feel free to add clarification sentences if necessary\n\n要点尽管许多LLM已通过安全对齐并配备输入检测机制，但其仍面临数据长尾分布带来的挑战。攻击者可利用密码、低资源语言等方法绕过安全防护，甚至通过遗传算法自动优化提示词，生成能突破安全限制的恶意输入。\n3.2.3 基于LLM的生成经过微调，LLM可以模拟攻击者，从而自动化生成对抗提示词。\n使用单一大模型\nMasterkey: Automated jailbreak across multiple large language model chatbots：\nMASTERKEY通过预训练和微调大语言模型构建而成，所用数据集包含各类原始及增强变体的对抗提示样本。受基于时间的SQL注入攻击启发，MASTERKEY深入剖析了大语言模型的内部防御策略（如Bing Chat和Bard等平台采用的实时语义分析与关键词检测防御机制）并据此设计攻击方案。\n\nHow johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms：\n从人类交流者的视角出发，首先基于社会科学研究构建了一套说服策略分类体系，随后运用上下文提示、微调式改写等多种方法，生成具有可解释性的说服性对抗提示（PAPs）。研究团队构建的训练数据以三元组形式组织：&lt;原始有害查询，分类体系中的策略技巧，对应的说服性对抗提示&gt;。这些数据将用于微调预训练大语言模型，最终生成一个自动化说服性改写器——只需输入有害查询和指定说服策略，该模型即可自动生成对应的说服性对抗提示。\n\nScalable and transferable black-box jailbreaks for language models via persona modulation：\n利用大语言模型助手自动生成人格调制攻击提示。攻击者只需向攻击用大语言模型提供包含对抗意图的初始提示，该模型便会自动搜索目标大语言模型易受攻击的人格特征，最终自动构建出能诱导目标模型扮演该特定人格的调制提示。\n\nExplore, establish, exploit: Red teaming language models from scratch：\n提出了一种无需预训练分类器的红队测试方法，首先构建行为分类系统：收集目标大语言模型的大量输出样本，由人类专家进行多维度标注，并训练能够准确反映人工评估结果的分类器。基于这些分类器提供的反馈信号，研究团队采用强化学习算法训练出攻击性大语言模型。\n\n\n使用多个大模型组成框架\nJailbreaking black box large language models in twenty queries:\nPAIR方法仅需对目标大语言模型进行黑盒访问即可生成越狱提示：先利用攻击者大语言模型不断查询目标模型，并基于反馈结果对越狱提示进行迭代优化更新。\n\nGuard: Role-playing to generate natural-language jailbreakings to test guideline adherence of large language models：\n设计了一个自动生成越狱提示的多智能体系统，通过不断查询目标大语言模型并优化提示语来实现攻击。在该系统中大语言模型分别担任生成器、翻译评估器、优化器。\n\nMart: Improving llm safety with multi-round automatic red-teaming：\n提出了一种将越狱攻击与安全对齐相集成的红队测试框架，通过联合优化实现双向提升。包含两个协同进化的过程：（1）攻击侧：生成有害提示尝试越狱目标模型，并根据目标模型的反馈持续优化攻击策略；（2）防御侧：目标模型通过对抗性提示的微调训练提升鲁棒性，形成防御能力迭代增强。\n\nEvil geniuses: Delving into the safety of llm-based agents：\nEvil Geniuses框架，通过红蓝对抗演练自动生成针对大语言模型智能体的越狱提示。\n\n\n结合其他方法的基于LLM的攻击\nGoal-oriented prompt attack and safety evaluation for llms：\n提出将对抗性提示分解为三个核心要素：攻击目标、内容主体和模板框架。研究团队针对不同攻击目标人工构建了大量内容素材和模板变体。随后通过以下自动化流程生成混合提示：（1）组合生成：大语言模型生成器随机组合预定义的内容与模板，产生混合提示；（2）效果评估：大语言模型评估器对生成的混合提示进行有效性判定。\n\nTree of attacks: Jailbreaking black-box llms automatically：\n提出了一种名为剪枝攻击树（TAP）的新型越狱方法。该方法采用迭代优化机制：（1）种子提示生成：从初始种子提示出发，系统自动生成改进变体；（2）劣质提示剪枝：通过评估机制淘汰效果不佳的提示变体；（3）有效性验证：保留的优质提示输入目标大语言模型进行攻击效果验证；（4）迭代优化：成功实现越狱的提示将作为新一代种子提示进入下一轮优化循环。\n\n\n要点利用大语言模型模拟攻击者的方法主要包含两大策略：一方面通过训练LLM直接扮演人类攻击者的角色，另一方面构建多LLM协同框架，使不同模型作为独立代理协作自动化生成越狱提示。此外，LLMs还与其他攻击技术（如情景嵌套和遗传算法）结合，显著提升攻击成功率。\n4 防御方法To Be Continued…\n","categories":["论文阅读","大模型","越狱"]}]