<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.23.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.json","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="朴素贝叶斯分类器公式推导与实现教程 一、朴素贝叶斯分类器基本概念 朴素贝叶斯分类器是一种基于贝叶斯定理与特征条件独立假设的分类方法。它之所以被称为”朴素”，是因为其假设特征之间是相互条件独立的，这一假设大大简化了计算复杂度。 朴素贝叶斯法通过训练数据集学习联合概率分布P(X, Y)，然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。 二、公式推导过程 2.1 条件概">
<meta property="og:type" content="article">
<meta property="og:title" content="朴素贝叶斯分类器公式推导与实现教程">
<meta property="og:url" content="http://example.com/posts/2510.002v1/index.html">
<meta property="og:site_name" content="Feixiang Shu&#39;s Blog">
<meta property="og:description" content="朴素贝叶斯分类器公式推导与实现教程 一、朴素贝叶斯分类器基本概念 朴素贝叶斯分类器是一种基于贝叶斯定理与特征条件独立假设的分类方法。它之所以被称为”朴素”，是因为其假设特征之间是相互条件独立的，这一假设大大简化了计算复杂度。 朴素贝叶斯法通过训练数据集学习联合概率分布P(X, Y)，然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。 二、公式推导过程 2.1 条件概">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-03T16:00:00.000Z">
<meta property="article:modified_time" content="2025-10-14T01:52:20.686Z">
<meta property="article:author" content="Feixiang Shu">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/posts/2510.002v1/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/posts/2510.002v1/","path":"/posts/2510.002v1/","title":"朴素贝叶斯分类器公式推导与实现教程"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>朴素贝叶斯分类器公式推导与实现教程 | Feixiang Shu's Blog</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous" defer></script>
<script src="/js/third-party/search/local-search.js" defer></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Feixiang Shu's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC%E4%B8%8E%E5%AE%9E%E7%8E%B0%E6%95%99%E7%A8%8B"><span class="nav-number">1.</span> <span class="nav-text">朴素贝叶斯分类器公式推导与实现教程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">1.1.</span> <span class="nav-text">一、朴素贝叶斯分类器基本概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B"><span class="nav-number">1.2.</span> <span class="nav-text">二、公式推导过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E4%B8%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1 条件概率与贝叶斯定理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E6%9C%80%E5%A4%A7%E5%8C%96%E5%87%86%E5%88%99"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2 后验概率最大化准则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E6%9D%A1%E4%BB%B6%E7%8B%AC%E7%AB%8B%E6%80%A7%E5%81%87%E8%AE%BE"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3 特征条件独立性假设</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E7%9A%84%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F"><span class="nav-number">1.2.4.</span> <span class="nav-text">2.4 完整的朴素贝叶斯公式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%E4%B8%8E%E5%B9%B3%E6%BB%91%E6%8A%80%E6%9C%AF"><span class="nav-number">1.3.</span> <span class="nav-text">三、参数估计与平滑技术</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="nav-number">1.3.1.</span> <span class="nav-text">3.1 极大似然估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%B9%B3%E6%BB%91"><span class="nav-number">1.3.2.</span> <span class="nav-text">3.2 拉普拉斯平滑</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9Bpython%E5%AE%9E%E7%8E%B0%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.4.</span> <span class="nav-text">四、Python实现示例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.4.1.</span> <span class="nav-text">4.1 文本分类示例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8scikit-learn%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.4.2.</span> <span class="nav-text">4.2 使用scikit-learn实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E4%B8%8D%E5%90%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%A4%84%E7%90%86"><span class="nav-number">1.5.</span> <span class="nav-text">五、不同数据类型的处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E7%89%B9%E5%BE%81%E6%9D%A1%E4%BB%B6%E7%8B%AC%E7%AB%8B%E6%80%A7%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="nav-number">1.6.</span> <span class="nav-text">六、特征条件独立性的影响</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%81%87%E8%AE%BE%E7%9A%84%E5%90%88%E7%90%86%E6%80%A7"><span class="nav-number">1.6.1.</span> <span class="nav-text">6.1 假设的合理性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%99%85%E5%BD%B1%E5%93%8D"><span class="nav-number">1.6.2.</span> <span class="nav-text">6.2 实际影响</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83%E6%80%BB%E7%BB%93"><span class="nav-number">1.7.</span> <span class="nav-text">七、总结</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Feixiang Shu"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Feixiang Shu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">38</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/sfmoraa" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;sfmoraa" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:sfx-sjtu@sjtu.edu.cn" title="E-Mail → mailto:sfx-sjtu@sjtu.edu.cn" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/posts/2510.002v1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Feixiang Shu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Feixiang Shu's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="朴素贝叶斯分类器公式推导与实现教程 | Feixiang Shu's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          朴素贝叶斯分类器公式推导与实现教程
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-10-04 00:00:00" itemprop="dateCreated datePublished" datetime="2025-10-04T00:00:00+08:00">2025-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-10-14 09:52:20" itemprop="dateModified" datetime="2025-10-14T09:52:20+08:00">2025-10-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0%E6%8F%90%E5%8D%87/" itemprop="url" rel="index"><span itemprop="name">学习提升</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0%E6%8F%90%E5%8D%87/%E5%9B%BE%E4%B8%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">图与大模型学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>14 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1
id="朴素贝叶斯分类器公式推导与实现教程">朴素贝叶斯分类器公式推导与实现教程</h1>
<h2 id="一朴素贝叶斯分类器基本概念">一、朴素贝叶斯分类器基本概念</h2>
<p>朴素贝叶斯分类器是一种基于贝叶斯定理与特征条件独立假设的分类方法。它之所以被称为”朴素”，是因为其假设特征之间是相互条件独立的，这一假设大大简化了计算复杂度。</p>
<p>朴素贝叶斯法通过训练数据集学习联合概率分布<span
class="math inline"><em>P</em>(<em>X</em>, <em>Y</em>)</span>，然后基于此模型，对给定的输入<span
class="math inline"><em>x</em></span>，利用贝叶斯定理求出后验概率最大的输出<span
class="math inline"><em>y</em></span>。</p>
<h2 id="二公式推导过程">二、公式推导过程</h2>
<h3 id="条件概率与贝叶斯定理">2.1 条件概率与贝叶斯定理</h3>
<p>首先回顾条件概率公式： <span class="math display">$$P(B|A) =
\frac{P(AB)}{P(A)}$$</span></p>
<p>由此可推导出贝叶斯定理： <span class="math display">$$P(A|B) =
\frac{P(B|A)P(A)}{P(B)}$$</span></p>
<table style="width:100%;">
<colgroup>
<col style="width: 16%" />
<col style="width: 11%" />
<col style="width: 15%" />
<col style="width: 56%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">成分</th>
<th style="text-align: left;">符号</th>
<th style="text-align: left;">名称</th>
<th style="text-align: left;">意义解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>更新后的信念</strong></td>
<td style="text-align: left;"><em>P</em>(<em>A</em>∥<em>B</em>)</td>
<td style="text-align: left;"><strong>后验概率</strong></td>
<td style="text-align: left;">在观察到新证据 <em>B</em> 之后，假设
<em>A</em>
为真的概率。这是我们最终想要求得的结果，它综合了我们先前的知识和新的证据。</td>
</tr>
<tr>
<td style="text-align: left;"><strong>当前证据的强度</strong></td>
<td style="text-align: left;"><em>P</em>(<em>B</em>∥<em>A</em>)</td>
<td style="text-align: left;"><strong>似然概率</strong></td>
<td style="text-align: left;">在假设 <em>A</em>
为真的条件下，观察到当前证据 <em>B</em>
的可能性有多大。它反映了证据与假设的匹配程度。</td>
</tr>
<tr>
<td style="text-align: left;"><strong>初始信念</strong></td>
<td style="text-align: left;"><em>P</em>(<em>A</em>)</td>
<td style="text-align: left;"><strong>先验概率</strong></td>
<td style="text-align: left;">在尚未观察到新证据 <em>B</em>
之前，我们对假设 <em>A</em>
为真的初始概率估计。这通常基于历史数据或领域知识。</td>
</tr>
<tr>
<td style="text-align: left;"><strong>证据的总体概率</strong></td>
<td style="text-align: left;"><em>P</em>(<em>B</em>)</td>
<td style="text-align: left;"><strong>边缘概率/证据</strong></td>
<td style="text-align: left;">证据 <em>B</em>
发生的总概率，通常通过考虑所有可能的情况（<em>A</em> 发生和 <em>A</em>
不发生）计算得出。它的作用是<strong>归一化</strong>，确保后验概率是一个有效的概率值（在0到1之间）。</td>
</tr>
</tbody>
</table>
<p>在分类问题中，我们关心的是给定特征<span
class="math inline"><em>X</em></span>情况下样本属于类别<span
class="math inline"><em>c</em><sub><em>k</em></sub></span>的概率： <span
class="math display">$$P(Y=c_k|X=x) =
\frac{P(X=x|Y=c_k)P(Y=c_k)}{P(X=x)}$$</span></p>
<h3 id="后验概率最大化准则">2.2 后验概率最大化准则</h3>
<p>贝叶斯分类器的目标是<strong>最小化期望风险</strong>(做出平均损失最小的决策)。采用0-1损失函数时，对某个样本
<em>x</em> 进行分类的<strong>条件风险</strong>，就是将其误分类的概率：
<span class="math display">$$R(c_i | x) = \sum_{k=1}^{K} \lambda(c_i,
c_k) P(c_k | x) = 1 - P(c_i | x)$$</span> 其中，<span
class="math display"><em>λ</em>(<em>c</em><sub><em>i</em></sub>, <em>c</em><sub><em>k</em></sub>)</span>
在 <em>i</em>=<em>k</em> 时为0，否则为1。</p>
<p>为了使<strong>总体风险</strong>最小化，我们需要对每个样本 <em>x</em>
逐个最小化其条件风险。这等价于<strong>最大化该样本属于正确类别的后验概率</strong>：
<span
class="math display"><em>f</em>(<em>x</em>) = arg min<sub><em>c</em><sub><em>i</em></sub></sub><em>R</em>(<em>c</em><sub><em>i</em></sub>|<em>x</em>) = arg min<sub><em>c</em><sub><em>i</em></sub></sub>[1 − <em>P</em>(<em>c</em><sub><em>i</em></sub>|<em>x</em>)] = arg max<sub><em>c</em><sub><em>i</em></sub></sub><em>P</em>(<em>c</em><sub><em>i</em></sub>|<em>x</em>)</span>
因此，<strong>后验概率最大化准则实质上就是在0-1损失函数下的最优决策，它保证了期望风险最小化</strong>。这一深刻的等价关系奠定了该准则在分类问题中的理论基础。</p>
<h3 id="特征条件独立性假设">2.3 特征条件独立性假设</h3>
<p>直接计算<span
class="math inline"><em>P</em>(<em>X</em> = <em>x</em>|<em>Y</em> = <em>c</em><sub><em>k</em></sub>)</span>面临组合爆炸问题（参数个数为<span
class="math inline">$K\prod_{j=1}^{n}S_j$</span>,<span
class="math inline"><em>K</em></span>为类别数，<span
class="math inline"><em>S</em><sub><em>j</em></sub></span>为第j个特征的可能的取值数）。为解决此问题，朴素贝叶斯引入了<strong>特征条件独立性假设</strong>：</p>
<p><span class="math display">$$P(X=x|Y=c_k) =
P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k) = \prod_{j=1}^{n}
P(X^{(j)}=x^{(j)}|Y=c_k)$$</span></p>
<h3 id="完整的朴素贝叶斯公式">2.4 完整的朴素贝叶斯公式</h3>
<p>将独立性假设代入贝叶斯公式： <span
class="math display">$$P(Y=c_k|X=x) = \frac{P(Y=c_k) \prod_{j=1}^{n}
P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_{k} P(Y=c_k) \prod_{j=1}^{n}
P(X^{(j)}=x^{(j)}|Y=c_k)}$$</span></p>
<p>由于分母对所有<span
class="math inline"><em>c</em><sub><em>k</em></sub></span>相同，朴素贝叶斯分类器可简化为：
<span class="math display">$$y = f(x) = argmax_{c_k} P(Y=c_k)
\prod_{j=1}^{n} P(X^{(j)}=x^{(j)}|Y=c_k)$$</span></p>
<h2 id="三参数估计与平滑技术">三、参数估计与平滑技术</h2>
<h3 id="极大似然估计">3.1 极大似然估计</h3>
<p>使用极大似然估计法估计先验概率和条件概率：</p>
<ul>
<li>先验概率：<span class="math inline">$P(Y=c_k) = \frac{\sum_{i=1}^{N}
I(y_i=c_k)}{N}$</span>，即用频率来估计概率</li>
<li>条件概率：<span class="math inline">$P(X^{(j)}=a_{jl}|Y=c_k) =
\frac{\sum_{i=1}^{N} I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^{N}
I(y_i=c_k)}$</span></li>
</ul>
<h3 id="拉普拉斯平滑">3.2 拉普拉斯平滑</h3>
<p>极大似然估计可能出现概率值为0的情况，解决方案是使用<strong>贝叶斯估计（拉普拉斯平滑）</strong>：</p>
<p><span class="math display">$$P(Y=c_k) = \frac{\sum_{i=1}^{N}
I(y_i=c_k) + \lambda}{N + K\lambda}$$</span></p>
<p><span class="math display">$$P(X^{(j)}=a_{jl}|Y=c_k) =
\frac{\sum_{i=1}^{N} I(x_i^{(j)}=a_{jl},y_i=c_k) +
\lambda}{\sum_{i=1}^{N} I(y_i=c_k) + S_j\lambda}$$</span></p>
<p>其中<span class="math inline"><em>λ</em> ≥ 0</span>，常取<span
class="math inline"><em>λ</em> = 1</span>。</p>
<h2 id="四python实现示例">四、Python实现示例</h2>
<h3 id="文本分类示例">4.1 文本分类示例</h3>
<p>下面是一个简单的朴素贝叶斯文本分类实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NaiveBayesClassifier</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, lambda_param=<span class="number">1</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.lambda_param = lambda_param  <span class="comment"># 拉普拉斯平滑参数</span></span><br><span class="line">        <span class="variable language_">self</span>.prior_prob = &#123;&#125;              <span class="comment"># 先验概率 P(c)</span></span><br><span class="line">        <span class="variable language_">self</span>.cond_prob = &#123;&#125;               <span class="comment"># 条件概率 P(x|c)</span></span><br><span class="line">        <span class="variable language_">self</span>.classes = []                 <span class="comment"># 类别列表</span></span><br><span class="line">        <span class="variable language_">self</span>.feature_total = &#123;&#125;           <span class="comment"># 每个类别下的特征总数</span></span><br><span class="line">        <span class="variable language_">self</span>.vocabulary = <span class="built_in">set</span>()           <span class="comment"># 所有出现过的特征集合</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;训练模型&quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.classes = <span class="built_in">list</span>(<span class="built_in">set</span>(y))</span><br><span class="line">        n_samples = <span class="built_in">len</span>(y)</span><br><span class="line">        n_classes = <span class="built_in">len</span>(<span class="variable language_">self</span>.classes)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算先验概率（使用拉普拉斯平滑）</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="variable language_">self</span>.classes:</span><br><span class="line">            count_c = <span class="built_in">sum</span>(<span class="number">1</span> <span class="keyword">for</span> label <span class="keyword">in</span> y <span class="keyword">if</span> label == c)</span><br><span class="line">            <span class="variable language_">self</span>.prior_prob[c] = (count_c + <span class="variable language_">self</span>.lambda_param) / (n_samples + n_classes * <span class="variable language_">self</span>.lambda_param)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 统计特征频率</span></span><br><span class="line">        feature_counts = &#123;c: defaultdict(<span class="built_in">int</span>) <span class="keyword">for</span> c <span class="keyword">in</span> <span class="variable language_">self</span>.classes&#125;</span><br><span class="line">        <span class="variable language_">self</span>.feature_total = &#123;c: <span class="number">0</span> <span class="keyword">for</span> c <span class="keyword">in</span> <span class="variable language_">self</span>.classes&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_samples):</span><br><span class="line">            features = X[i]</span><br><span class="line">            label = y[i]</span><br><span class="line">            <span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">                feature_counts[label][feature] += <span class="number">1</span></span><br><span class="line">                <span class="variable language_">self</span>.feature_total[label] += <span class="number">1</span></span><br><span class="line">                <span class="variable language_">self</span>.vocabulary.add(feature)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算条件概率（使用拉普拉斯平滑）</span></span><br><span class="line">        vocab_size = <span class="built_in">len</span>(<span class="variable language_">self</span>.vocabulary)</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="variable language_">self</span>.classes:</span><br><span class="line">            <span class="variable language_">self</span>.cond_prob[c] = &#123;&#125;</span><br><span class="line">            total_features_c = <span class="variable language_">self</span>.feature_total[c]</span><br><span class="line">            <span class="keyword">for</span> feature <span class="keyword">in</span> <span class="variable language_">self</span>.vocabulary:</span><br><span class="line">                count_feature = feature_counts[c][feature]  <span class="comment"># 如果feature没出现过，count_feature为0</span></span><br><span class="line">                <span class="variable language_">self</span>.cond_prob[c][feature] = (count_feature + <span class="variable language_">self</span>.lambda_param) / \</span><br><span class="line">                                            (total_features_c + vocab_size * <span class="variable language_">self</span>.lambda_param)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;预测新样本&quot;&quot;&quot;</span></span><br><span class="line">        predictions = []</span><br><span class="line">        vocab_size = <span class="built_in">len</span>(<span class="variable language_">self</span>.vocabulary)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> sample <span class="keyword">in</span> X:</span><br><span class="line">            max_prob = -<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">            predicted_class = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> <span class="variable language_">self</span>.classes:</span><br><span class="line">                log_prob = np.log(<span class="variable language_">self</span>.prior_prob[c])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> feature <span class="keyword">in</span> sample:</span><br><span class="line">                    <span class="keyword">if</span> feature <span class="keyword">in</span> <span class="variable language_">self</span>.cond_prob[c]:</span><br><span class="line">                        log_prob += np.log(<span class="variable language_">self</span>.cond_prob[c][feature])</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="comment"># 处理未出现的特征（使用拉普拉斯平滑）</span></span><br><span class="line">                        total_features_c = <span class="variable language_">self</span>.feature_total[c]</span><br><span class="line">                        prob = <span class="variable language_">self</span>.lambda_param / (total_features_c + vocab_size * <span class="variable language_">self</span>.lambda_param)</span><br><span class="line">                        log_prob += np.log(prob)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> log_prob &gt; max_prob:</span><br><span class="line">                    max_prob = log_prob</span><br><span class="line">                    predicted_class = c</span><br><span class="line"></span><br><span class="line">            predictions.append(predicted_class)</span><br><span class="line">        <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataset</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建简单的文本分类数据集&quot;&quot;&quot;</span></span><br><span class="line">    X = [</span><br><span class="line">        [<span class="string">&#x27;my&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;has&#x27;</span>, <span class="string">&#x27;flea&#x27;</span>, <span class="string">&#x27;problems&#x27;</span>, <span class="string">&#x27;help&#x27;</span>, <span class="string">&#x27;please&#x27;</span>],</span><br><span class="line">        [<span class="string">&#x27;maybe&#x27;</span>, <span class="string">&#x27;not&#x27;</span>, <span class="string">&#x27;take&#x27;</span>, <span class="string">&#x27;him&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;park&#x27;</span>, <span class="string">&#x27;stupid&#x27;</span>],</span><br><span class="line">        [<span class="string">&#x27;my&#x27;</span>, <span class="string">&#x27;dalmation&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;so&#x27;</span>, <span class="string">&#x27;cute&#x27;</span>, <span class="string">&#x27;I&#x27;</span>, <span class="string">&#x27;love&#x27;</span>, <span class="string">&#x27;him&#x27;</span>],</span><br><span class="line">        [<span class="string">&#x27;stop&#x27;</span>, <span class="string">&#x27;posting&#x27;</span>, <span class="string">&#x27;stupid&#x27;</span>, <span class="string">&#x27;worthless&#x27;</span>, <span class="string">&#x27;garbage&#x27;</span>],</span><br><span class="line">        [<span class="string">&#x27;mr&#x27;</span>, <span class="string">&#x27;licks&#x27;</span>, <span class="string">&#x27;ate&#x27;</span>, <span class="string">&#x27;my&#x27;</span>, <span class="string">&#x27;steak&#x27;</span>, <span class="string">&#x27;how&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;stop&#x27;</span>, <span class="string">&#x27;him&#x27;</span>],</span><br><span class="line">        [<span class="string">&#x27;quit&#x27;</span>, <span class="string">&#x27;buying&#x27;</span>, <span class="string">&#x27;worthless&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;food&#x27;</span>, <span class="string">&#x27;stupid&#x27;</span>]</span><br><span class="line">    ]</span><br><span class="line">    y = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">X_train, y_train = create_dataset()</span><br><span class="line">nb_classifier = NaiveBayesClassifier(lambda_param=<span class="number">1</span>)</span><br><span class="line">nb_classifier.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试新样本</span></span><br><span class="line">test_sample = [[<span class="string">&#x27;love&#x27;</span>, <span class="string">&#x27;my&#x27;</span>, <span class="string">&#x27;dalmation&#x27;</span>]]</span><br><span class="line">prediction = nb_classifier.predict(test_sample)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;预测结果: <span class="subst">&#123;prediction[<span class="number">0</span>]&#125;</span>&quot;</span>)  <span class="comment"># 输出0（非侮辱性）</span></span><br></pre></td></tr></table></figure>
<h3 id="使用scikit-learn实现">4.2 使用scikit-learn实现</h3>
<p>对于实际应用，推荐使用scikit-learn库中的朴素贝叶斯实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB, GaussianNB, BernoulliNB</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建文本数据向量化表示</span></span><br><span class="line">vectorizer = CountVectorizer()</span><br><span class="line">X_vectorized = vectorizer.fit_transform([<span class="string">&#x27; &#x27;</span>.join(doc) <span class="keyword">for</span> doc <span class="keyword">in</span> X_train])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用多项式朴素贝叶斯（适用于文本数据）</span></span><br><span class="line">mnb = MultinomialNB(alpha=<span class="number">1.0</span>)  <span class="comment"># alpha对应拉普拉斯平滑参数</span></span><br><span class="line">mnb.fit(X_vectorized, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">test_text = [<span class="string">&#x27; &#x27;</span>.join([<span class="string">&#x27;love&#x27;</span>, <span class="string">&#x27;my&#x27;</span>, <span class="string">&#x27;dalmation&#x27;</span>])]</span><br><span class="line">test_vectorized = vectorizer.transform(test_text)</span><br><span class="line">prediction = mnb.predict(test_vectorized)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Scikit-learn预测结果: <span class="subst">&#123;prediction[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line">y_pred = mnb.predict(X_vectorized)</span><br><span class="line">accuracy = accuracy_score(y_train, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;模型准确率: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="五不同数据类型的处理">五、不同数据类型的处理</h2>
<p>朴素贝叶斯有多种变体，适用于不同类型的数据：</p>
<ol type="1">
<li><strong>高斯朴素贝叶斯</strong>：假设特征服从正态分布，适用于连续特征</li>
<li><strong>多项式朴素贝叶斯</strong>：适用于离散特征（如文本分类中的词频）</li>
<li><strong>伯努利朴素贝叶斯</strong>：适用于二值特征</li>
</ol>
<h2 id="六特征条件独立性的影响">六、特征条件独立性的影响</h2>
<h3 id="假设的合理性">6.1 假设的合理性</h3>
<p>特征条件独立性假设在现实中<strong>往往不成立</strong>，例如在文本分类中，词语之间通常存在关联。但这假设带来了两个重要好处：</p>
<ol type="1">
<li><strong>计算简化</strong>：将<span
class="math inline"><em>O</em>(2<sup><em>n</em></sup>)</span>的参数空间减少到<span
class="math inline"><em>O</em>(<em>n</em>)</span></li>
<li><strong>数据效率</strong>：减少了对大量训练数据的需求</li>
</ol>
<h3 id="实际影响">6.2 实际影响</h3>
<p>尽管有关联性假设，朴素贝叶斯在实践中的表现却经常出人意料地好，原因包括：</p>
<ul>
<li>分类决策只依赖于<strong>概率的排序</strong>而非精确值</li>
<li>当特征相关性较小时，性能接近最优贝叶斯分类器</li>
<li>对文本分类等许多实际任务效果良好</li>
</ul>
<h2 id="七总结">七、总结</h2>
<p>朴素贝叶斯分类器通过以下步骤实现： 1. 基于训练数据估计先验概率<span
class="math inline"><em>P</em>(<em>Y</em> = <em>c</em><sub><em>k</em></sub>)</span>和条件概率<span
class="math inline"><em>P</em>(<em>X</em><sup>(<em>j</em>)</sup>|<em>Y</em> = <em>c</em><sub><em>k</em></sub>)</span>
2. 对给定的新实例<span
class="math inline"><em>x</em></span>，计算每个类别的后验概率分子<span
class="math inline">$P(Y=c_k) \prod_{j=1}^{n}
P(X^{(j)}=x^{(j)}|Y=c_k)$</span> 3.
选择使后验概率最大的类别作为预测结果</p>
<p>虽然其”朴素”的独立性假设在现实中往往不成立，但朴素贝叶斯仍因其简单高效、需要少量训练数据、对缺失数据不敏感等优点，在实践中得到广泛应用，特别是在文本分类和垃圾邮件过滤等领域。</p>
<p>通过本教程，您应该能够完整理解朴素贝叶斯的数学原理、推导过程和实践应用。建议结合具体数据集进一步练习，以加深理解。</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/2510.001v1/" rel="prev" title="概率分布教程：从数学基础到机器学习应用">
                  <i class="fa fa-angle-left"></i> 概率分布教程：从数学基础到机器学习应用
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/2510.003v1/" rel="next" title="使用NumPy实现高斯朴素贝叶斯分类器：鸢尾花数据集全流程教程">
                  使用NumPy实现高斯朴素贝叶斯分类器：鸢尾花数据集全流程教程 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Feixiang Shu</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">206k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">6:14</span>
  </span>
</div>

<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("05/21/2025 10:00:00"); 
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>
    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
