<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.23.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.json","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js" defer></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="Feixiang Shu&#39;s Blog">
<meta property="og:url" content="http://example.com/null-index/page/8/index.html">
<meta property="og:site_name" content="Feixiang Shu&#39;s Blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Feixiang Shu">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/null-index/page/8/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"null-index/page/8/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Feixiang Shu's Blog</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous" defer></script>
<script src="/js/third-party/search/local-search.js" defer></script>







  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Feixiang Shu's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Feixiang Shu"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Feixiang Shu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/sfmoraa" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;sfmoraa" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:sfx-sjtu@sjtu.edu.cn" title="E-Mail → mailto:sfx-sjtu@sjtu.edu.cn" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/posts/2505.002v1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Feixiang Shu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Feixiang Shu's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Feixiang Shu's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2505.002v1/" class="post-title-link" itemprop="url">TinyLLM学习日记</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-05-22 00:00:00" itemprop="dateCreated datePublished" datetime="2025-05-22T00:00:00+08:00">2025-05-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-05-24 13:00:17" itemprop="dateModified" datetime="2025-05-24T13:00:17+08:00">2025-05-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/LLM%E5%AE%9E%E8%B7%B5/" itemprop="url" rel="index"><span itemprop="name">LLM实践</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/LLM%E5%AE%9E%E8%B7%B5/TinyLLM/" itemprop="url" rel="index"><span itemprop="name">TinyLLM</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>我希望通过训练一个TinyLLM来打好大模型训练的基础，过程中会遇到很多问题，因此在这里记录学习日记。比较重要的是，要学习理解好一些封装好的接口的使用，避免知其然不知其所以然。</p>
<p>本部分的教程使用的是<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/tiny-universe">datawhalechina&#x2F;tiny-universe: 《大模型白盒子构建指南》：一个全手搓的Tiny-Universe</a>中的<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/tiny-universe/tree/main/content/TinyLLM">TinyLLM</a>部分，不会再对原教程赘述，只记录相关探索的笔记。</p>
<h1 id="Step-1-训练Tokenizer"><a href="#Step-1-训练Tokenizer" class="headerlink" title="Step 1: 训练Tokenizer"></a>Step 1: 训练Tokenizer</h1><h2 id="SentencePiece库的使用"><a href="#SentencePiece库的使用" class="headerlink" title="SentencePiece库的使用"></a>SentencePiece库的使用</h2><h3 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h3><p>TinyLLM使用了 <code>SentencePiece</code> 库来训练自定义的 <code>Tokenizer</code>，我希望增进对其的理解，参考资料：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/630696264">大模型词表扩充必备工具SentencePiece - 知乎</a>。</p>
<ul>
<li><p>Tokenizer有三种粒度：word&#x2F;character&#x2F;subword</p>
</li>
<li><p>subword平衡了两种方法，常见的子词算法有Byte-Pair Encoding (BPE) &#x2F; Byte-level BPE（BBPE）、Unigram LM、WordPiece、SentencePiece等。</p>
<ul>
<li>BPE，即字节对编码。其核心思想是从字母开始，不断找词频最高、且连续的两个token合并，直到达到目标词数。</li>
<li>BBPE的核心思想是将BPE从字符级别扩展到子节（Byte）级别。BPE的一个问题是如果遇到了unicode编码，基本字符集可能会很大。BBPE就是以一个字节为一种“字符”，不管实际字符集用了几个字节来表示一个字符。这样的话，基础字符集的大小就锁定在了256（2^8）。采用BBPE的好处是可以跨语言共用词表，显著压缩词表的大小。而坏处就是，对于类似中文这样的语言，一段文字的序列长度会显著增长。因此，BBPE based模型可能比BPE based模型表现的更好。然而，BBPE sequence比起BPE来说略长，这也导致了更长的训练&#x2F;推理时间。BBPE其实与BPE在实现上并无大的不同，只不过基础词表使用256的字节集。</li>
</ul>
</li>
</ul>
<h3 id="SentencePiece-特性"><a href="#SentencePiece-特性" class="headerlink" title="SentencePiece 特性"></a>SentencePiece 特性</h3><ul>
<li>固定最终词汇表大小</li>
<li>使用原始句子训练</li>
<li>空格被视为基本符号 “▁” ，因此可以无歧义地对文本进行detokenize</li>
</ul>
<h3 id="SentencePiece-实验"><a href="#SentencePiece-实验" class="headerlink" title="SentencePiece 实验"></a>SentencePiece 实验</h3><p>使用一个简单的示例进行测试：“aa bb cc aab abbd bb.”</p>
<p>测试代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import sentencepiece as spm</span><br><span class="line">dataset_path = &#x27;./demo.txt&#x27;</span><br><span class="line">vocab_size=10</span><br><span class="line">spm.SentencePieceTrainer.train(input=dataset_path,model_type=&quot;bpe&quot;, model_prefix=&#x27;demo&#x27;, vocab_size=vocab_size)</span><br><span class="line"></span><br><span class="line">sp = spm.SentencePieceProcessor()</span><br><span class="line">sp.load(&#x27;demo.model&#x27;)</span><br><span class="line"></span><br><span class="line">text=&#x27;aa bb cc aab abbd bb.&#x27;</span><br><span class="line">print(sp.encode_as_pieces(text))</span><br><span class="line">print(sp.encode_as_ids(text))</span><br><span class="line"></span><br><span class="line">for i in range(vocab_size):</span><br><span class="line">    print(i,sp.id_to_piece(i))</span><br></pre></td></tr></table></figure>

<p>使用该代码可以看到分词器对这一简单句子的分词结果。</p>
<p>设置vocab_size小于9时，会报错</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: Internal: src/trainer_interface.cc(582) [(static_cast&lt;int&gt;(required_chars_.size() + meta_pieces_.size())) &lt;= (trainer_spec_.vocab_size())] Vocabulary size is smaller than required_chars. 8 vs 9. Increase vocab_size or decrease character_coverage with --character_coverage option.</span><br></pre></td></tr></table></figure>

<p>这是因为SentencePieceTrainer会自动添加未知符： <unk>、BOS：&lt;s&gt;、EOS：&lt;&#x2F;s&gt;、▁，加上这个例子本来的5个字符，需要至少9个字符才能分词。</p>
<p>而设置的上限即分词算法能计算到最大标记总数，例如，考虑一个简单的例子“ab ac bc cd de”，其上限是：字符总数（5）+</p>
<p>”▁？“型（4）+”▁？？“型（5）+”？？“型（5）+自动添加（4）&#x3D;23。</p>
<p>一般遇到设定词典大小过大的问题时，可能是数据不够丰富导致的，这时可以选择增加数据或者减少词典大小。</p>
<h3 id="分词器训练与使用"><a href="#分词器训练与使用" class="headerlink" title="分词器训练与使用"></a>分词器训练与使用</h3><ul>
<li>训练：（参数文档参考<a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece/blob/master/doc/options.md">sentencepiece&#x2F;doc&#x2F;options.md at master · google&#x2F;sentencepiece</a>）</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">spm.SentencePieceTrainer.train(</span><br><span class="line">        input=tiny_file,         # 输入文件为之前生成的 tiny.txt</span><br><span class="line">        model_prefix=prefix,     # 模型前缀路径</span><br><span class="line">        model_type=&quot;bpe&quot;,        # 使用 Byte-Pair Encoding (BPE) 训练分词器</span><br><span class="line">        vocab_size=vocab_size,   # 词汇表大小</span><br><span class="line">        self_test_sample_size=0, # 自测样本大小设置为 0</span><br><span class="line">        input_format=&quot;text&quot;,     # 输入文件格式为纯文本</span><br><span class="line">        character_coverage=1.0,  # 覆盖所有字符（包括非常见字符）</span><br><span class="line">        num_threads=os.cpu_count(),  # 使用 CPU 的线程数</span><br><span class="line">        split_digits=True,       # 拆分数字</span><br><span class="line">        allow_whitespace_only_pieces=True,  # 允许仅由空格组成的词元</span><br><span class="line">        byte_fallback=True,      # 启用字节级回退</span><br><span class="line">        unk_surface=r&quot; \342\201\207 &quot;,  # UNK token 表示未知字符的方式</span><br><span class="line">        normalization_rule_name=&quot;identity&quot;  # 使用“identity”归一化规则</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<ul>
<li>加载：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sp_model = SentencePieceProcessor(model_file=model_path)</span><br></pre></td></tr></table></figure>

<ul>
<li>编码：(s：str)</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sp_model.encode(s)</span><br></pre></td></tr></table></figure>

<ul>
<li>解码：(t: List[int])</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sp_model.decode(t)</span><br></pre></td></tr></table></figure>


<h1 id="Step-2-数据预处理"><a href="#Step-2-数据预处理" class="headerlink" title="Step 2: 数据预处理"></a>Step 2: 数据预处理</h1><h2 id="functools-partial"><a href="#functools-partial" class="headerlink" title="functools.partial"></a>functools.partial</h2><p><code>functools.partial</code> 是 Python 标准库中 <code>functools</code> 模块提供的一个高阶函数，主要用于部分应用函数参数。它允许固定函数的部分参数，生成一个新的简化版函数，从而减少后续调用时的参数传递量。</p>
<p>代码将<code>process_shard(args, vocab_size, tokenizer_model_path)</code></p>
<p>封装为<code>fun = partial(process_shard, vocab_size=vocab_size, tokenizer_model_path=TOKENIZER_MODEL)</code></p>
<p>则后续调用时形如<code>fun((0,&#39;path&#39;))</code>，传入一个元组</p>
<h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>将文本数据使用Step1训练的分词器转换为数字序列，并编码为可训练的格式（为每一段文本添加BOS），最后以二进制形式保存</p>
<h2 id="加载已预处理好的数据集"><a href="#加载已预处理好的数据集" class="headerlink" title="加载已预处理好的数据集"></a>加载已预处理好的数据集</h2><p>TinyLLM中设计了一个 <code>PretokDataset</code> 类</p>
<p>核心加载数据的代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">while True:</span><br><span class="line">    # 随机打乱分片文件</span><br><span class="line">    rng.shuffle(shard_filenames)</span><br><span class="line">    for shard in shard_filenames:</span><br><span class="line">        # 使用 memmap 读取文件，使得数据留在磁盘上，减少内存占用</span><br><span class="line">        m = np.memmap(shard, dtype=np.uint16, mode=&quot;r&quot;)</span><br><span class="line">        # 计算该分片中的批次数量</span><br><span class="line">        num_batches = len(m) // self.max_seq_len</span><br><span class="line">        num_batches -= 1  # 去掉最后一个不完整的批次</span><br><span class="line">        assert num_batches &gt; 0, &quot;这个分片文件太小了？请检查。&quot;</span><br><span class="line">        # 随机打乱批次索引</span><br><span class="line">        ixs = list(range(num_batches))</span><br><span class="line">        rng.shuffle(ixs)</span><br><span class="line">        # 对每个批次生成输入 x 和目标输出 y</span><br><span class="line">        for ix in ixs:</span><br><span class="line">            start = ix * self.max_seq_len  # 批次起始索引</span><br><span class="line">            end = start + self.max_seq_len + 1  # 批次结束索引</span><br><span class="line">            # 将数据转换为 NumPy 数组并拷贝到 RAM 中</span><br><span class="line">            chunk = torch.from_numpy((m[start:end]).astype(np.int64))</span><br><span class="line">            # 模型输入 x 是当前批次的前 max_seq_len 个词元</span><br><span class="line">            x = chunk[:-1]</span><br><span class="line">            # 模型输出 y 是下一个词元</span><br><span class="line">            y = chunk[1:]</span><br><span class="line">            # 生成 x, y 对</span><br><span class="line">            yield x, y</span><br></pre></td></tr></table></figure>

<p>可以看出这里使用的是步长与窗口大小相等的滑动窗口采样方法，之后可以尝试修改这部分数据加载机制以更大程度地利用数据。</p>
<h1 id="Step-3-训练模型"><a href="#Step-3-训练模型" class="headerlink" title="Step 3: 训练模型"></a>Step 3: 训练模型</h1><p>TInyLLM使用的模型是与 LLaMA2 结构相同的 Decoder-only Transformer 模型，此部分根据源码进行解读分析。</p>
<p>在最基本的大模型架构基础上，使用了以下策略：</p>
<h2 id="对残差投影进行特殊的缩放初始化"><a href="#对残差投影进行特殊的缩放初始化" class="headerlink" title="对残差投影进行特殊的缩放初始化"></a>对残差投影进行特殊的缩放初始化</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for pn, p in self.named_parameters():</span><br><span class="line">    if pn.endswith(&#x27;w3.weight&#x27;) or pn.endswith(&#x27;wo.weight&#x27;):</span><br><span class="line">        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * args.n_layers))</span><br></pre></td></tr></table></figure>

<p>这是对DecoderLayer内的MLP层的第三层线性变换和Attention层的输出权重矩阵进行放缩</p>
<h2 id="旋转编码"><a href="#旋转编码" class="headerlink" title="旋转编码"></a>旋转编码</h2><p>参考<a target="_blank" rel="noopener" href="https://www.zhihu.com/tardis/bd/art/647109286">十分钟读懂旋转编码（RoPE）</a></p>
<p>TinyLLM这里的实现与LLAMA里的一致，之后再专门研究学习一下编码。</p>
<h2 id="学习率调整"><a href="#学习率调整" class="headerlink" title="学习率调整"></a>学习率调整</h2><p>包括线性预热、余弦退火和最小学习率限制。</p>
<h2 id="自动混合精度训练"><a href="#自动混合精度训练" class="headerlink" title="自动混合精度训练"></a>自动混合精度训练</h2><p>参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38253797/article/details/116210911">【Trick2】torch.cuda.amp自动混合精度训练 —— 节省显存并加快推理速度_torch.cuda.amp.gradscaler()-CSDN博客</a>。</p>
<p>因为在某些上下文中torch.FloatTensor有优势，有的torch.HalfTensor有优势。动态估计的原理就是在不出现inf或者NaN梯度值的情况下尽可能增大scaler的值。在每次scaler.step(optimizer)中，都会检查是否有inf或NaN的梯度出现：</p>
<ol>
<li>如果出现了inf或者NaN，scaler.step(optimizer)会忽略此次的权重更新（optimizer.step() )，并且将scaler的大小缩小（乘上backoff_factor）；</li>
<li>如果没有出现inf或者NaN，那么权重正常更新，并且当连续多次（growth_interval指定）没有出现inf或者NaN，则scaler.update()会将scaler的大小增加（乘上growth_factor）。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 实例化一个GradScaler对象</span><br><span class="line">scaler = amp.GradScaler(enabled=True)</span><br><span class="line"># 将梯度放大 防止梯度消失</span><br><span class="line">scaler.scale(loss).backward()</span><br><span class="line"># 更新优化器和梯度缩放器</span><br><span class="line">scaler.step(optimizer)</span><br><span class="line">scaler.update()</span><br></pre></td></tr></table></figure>



<h1 id="Step-4-使用模型生成文本"><a href="#Step-4-使用模型生成文本" class="headerlink" title="Step 4: 使用模型生成文本"></a>Step 4: 使用模型生成文本</h1><p>推理时，为提示词加上BOS，然后逐个字符生成，可以使用<em>temperature</em>、top_k来控制生成的随机性。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/null-index/page/7/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/null-index/">1</a><span class="space">&hellip;</span><a class="page-number" href="/null-index/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/null-index/page/9/">9</a><a class="page-number" href="/null-index/page/10/">10</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/null-index/page/9/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Feixiang Shu</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">43k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">1:18</span>
  </span>
</div>

<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("05/21/2025 10:00:00"); 
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>
    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
